{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdeb5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"smolagents[litellm]\" google-search-results markdownify beautifulsoup4 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6a723",
   "metadata": {},
   "source": [
    "### Open Deep Reseach Agent ì¬í˜„í•´ë³´ê¸°!\n",
    "\n",
    "ì´ ì‹¤ìŠµì—ì„œëŠ” Hugging Face smolagents ì˜ˆì œì¸ open_deep_researchì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ë©°, ì—¬ëŸ¬ ë„êµ¬ì™€ ë‘ ì¢…ë¥˜ì˜ Agent(Worker, Manager)ë¥¼ ì¡°í•©í•˜ì—¬ ì •êµí•œ ë¦¬ì„œì¹˜ Agentë¥¼ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "ìƒíƒœë¥¼ ê°€ì§€ëŠ”(Stateful) ë¸Œë¼ìš°ì €ì™€ ì´ë¥¼ ì œì–´í•˜ëŠ” **ë„êµ¬ í´ë˜ìŠ¤(Tool Class)**ë¥¼ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ToolCallingAgent(Worker)ì™€ CodeAgent(Manager)ì˜ ì—­í• ì„ ì´í•´í•˜ê³  Agent íŒ€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œ í”„ë¡œì íŠ¸ ìˆ˜ì¤€ì˜ Agent ì•„í‚¤í…ì²˜ë¥¼ ê²½í—˜í•˜ê³ , ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì„ ë°°ì›ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1be6c",
   "metadata": {},
   "source": [
    "ë¦¬ì„œì¹˜ AgentëŠ” ë‘ ê°€ì§€ í•µì‹¬ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "OpenAI API Key: Agentì˜ ë‘ë‡Œ ì—­í• ì„ í•˜ëŠ” LLMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "\n",
    "Serper API Key: êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. (serper.devì—ì„œ ë¬´ë£Œ í‚¤ë¥¼ ë°œê¸‰, í•˜ë£¨ 150íšŒ ì‚¬ìš© ê°€ëŠ¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb00c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ or \"SERPER_API_KEY\" not in os.environ:\n",
    "    print(\"âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”!\")\n",
    "else:\n",
    "    print(\"âœ… API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# Serper API í‚¤ë¥¼ SerpAPI í‚¤ë¡œ ì¸ì‹í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ ì´ë¦„ì„ ë³€ê²½í•´ì¤ë‹ˆë‹¤.\n",
    "# (smolagentsì˜ GoogleSearchToolì´ ë‚´ë¶€ì ìœ¼ë¡œ ì´ ë³€ìˆ˜ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.)\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.environ[\"SERPER_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3660154",
   "metadata": {},
   "source": [
    "### ë‹¨ê³„ 1: Agentì˜ í•µì‹¬ ë„êµ¬ - SimpleTextBrowser í´ë˜ìŠ¤ ì´í•´í•˜ê¸°\n",
    "\n",
    "ë‹¨ìˆœí•œ ê²€ìƒ‰ í•¨ìˆ˜ë¥¼ ë„˜ì–´, open_deep_research ì˜ˆì œëŠ” **ìƒíƒœë¥¼ ê¸°ì–µí•˜ëŠ” SimpleTextBrowser**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§ˆì¹˜ ì‚¬ëŒì´ ì›¹ì„œí•‘ í•˜ë“¯, í˜„ì¬ ì–´ë–¤ í˜ì´ì§€ì— ìˆëŠ”ì§€, í˜ì´ì§€ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ì§€ ë“±ì„ ê¸°ì–µí•˜ëŠ” ë˜‘ë˜‘í•œ ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‰ Browser navigating to: about:blank\n",
      "ğŸ‘‰ Browser navigating to: https://www.google.com\n",
      "\n",
      "--- í˜ì´ì§€ ì œëª©: Google ---\n",
      "[Google ì •ë³´](https://about.google/?fg=1&utm_source=google-KR&utm_medium=referral&utm_campaign=hp-header)[ìŠ¤í† ì–´](https://store.google.com/KR?utm_source=hp_header&utm_medium=google_ooo&utm_campaign=GS100042&hl=ko-KR)\n",
      "\n",
      "[Gmail](https://mail.google.com/mail/&ogbl)\n",
      "\n",
      "[ì´ë¯¸ì§€](https://www.google.com/imghp?hl=ko&ogbl)\n",
      "\n",
      "[ë¡œê·¸ì¸](https://accounts.google.com/ServiceLogin?hl=ko&passive=true&continue=https://www.google.com/&ec=futura_exp_og_so_72776762_e)\n",
      "\n",
      "![](/tia/tia.png)\n",
      "\n",
      "- ![]()\n",
      "\n",
      "  ![]()![]()\n",
      "\n",
      "  ---\n",
      "\n",
      "  ë”ë³´ê¸°\n",
      "\n",
      "  ì‚­ì œ\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from markdownify import markdownify as md\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SimpleTextBrowser:\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì›¹ ë¸Œë¼ìš°ì € í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self, start_page: str = \"about:blank\"):\n",
    "        self.history = []\n",
    "        self.page_content = \"\"\n",
    "        self.page_title = \"\"\n",
    "        self.visit_page(start_page)\n",
    "\n",
    "    @property\n",
    "    def address(self) -> str:\n",
    "        \"\"\"í˜„ì¬ í˜ì´ì§€ì˜ ì£¼ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return self.history[-1] if self.history else \"\"\n",
    "\n",
    "    def visit_page(self, url: str) -> str:\n",
    "        \"\"\"ì£¼ì–´ì§„ URLì„ ë°©ë¬¸í•˜ê³  í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "        print(f\"ğŸ‘‰ Browser navigating to: {url}\")\n",
    "        self.history.append(url)\n",
    "        \n",
    "        if url == \"about:blank\":\n",
    "            self.page_content = \"Blank page\"\n",
    "            self.page_title = \"Blank\"\n",
    "            return self.page_content\n",
    "\n",
    "        try:\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status() # ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ\n",
    "            \n",
    "            # HTMLì„ Markdownìœ¼ë¡œ ë³€í™˜\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            self.page_title = soup.title.string if soup.title else \"No Title\"\n",
    "            \n",
    "            # body íƒœê·¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ ë” ê¹”ë”í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "            body = soup.find('body')\n",
    "            if body:\n",
    "                self.page_content = md(str(body))\n",
    "            else:\n",
    "                self.page_content = md(response.text)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.page_content = f\"Error: Could not load page at {url}. Reason: {e}\"\n",
    "            self.page_title = \"Error\"\n",
    "            \n",
    "        return self.page_content\n",
    "\n",
    "# ë¸Œë¼ìš°ì €ê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•´ë´…ì‹œë‹¤.\n",
    "browser_instance = SimpleTextBrowser()\n",
    "content = browser_instance.visit_page(\"https://www.google.com\")\n",
    "print(f\"\\n--- í˜ì´ì§€ ì œëª©: {browser_instance.page_title} ---\")\n",
    "# ë‚´ìš©ì´ ë„ˆë¬´ ê¸°ë¯€ë¡œ ì• 500ìë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3faaac0",
   "metadata": {},
   "source": [
    "### ë‹¨ê³„ 2: Browserë¥¼ ì œì–´í•˜ëŠ” 'ë„êµ¬ í´ë˜ìŠ¤' ë§Œë“¤ê¸°\n",
    "ì´ì œ SimpleTextBrowser ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œì–´í•  Toolë“¤ì„ ë§Œë“­ë‹ˆë‹¤. smolagentsì˜ Tool í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„, Agentê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í‘œì¤€í™”ëœ ë„êµ¬ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d9472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¸Œë¼ìš°ì € ì œì–´ ë„êµ¬ë“¤ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: ['web_search', 'visit_page']\n"
     ]
    }
   ],
   "source": [
    "from smolagents import Tool, GoogleSearchTool\n",
    "\n",
    "# 1. í˜ì´ì§€ ë°©ë¬¸ ë„êµ¬\n",
    "class VisitTool(Tool):\n",
    "    name = \"visit_page\"\n",
    "    description = \"ì£¼ì–´ì§„ URLì˜ ì›¹í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\n",
    "    inputs = {\"url\": {\"type\": \"string\", \"description\": \"ë°©ë¬¸í•  ì›¹í˜ì´ì§€ì˜ ì ˆëŒ€ ì£¼ì†Œ\"}}\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, browser: SimpleTextBrowser):\n",
    "        super().__init__()\n",
    "        self.browser = browser\n",
    "\n",
    "    def forward(self, url: str) -> str:\n",
    "        return self.browser.visit_page(url)\n",
    "\n",
    "# 2. ìš°ë¦¬ëŠ” GoogleSearchToolì€ smolagentsì—ì„œ ë°”ë¡œ ê°€ì ¸ì™€ ì‚¬ìš©!!\n",
    "#    -> ì´ ë„êµ¬ëŠ” SERPER_API_KEY(SERPAPI_API_KEY)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ìƒì„±í•œ ë„êµ¬ë“¤ì„ ì¸ìŠ¤í„´ìŠ¤í™” í•©ë‹ˆë‹¤.\n",
    "# ì´ ë•Œ, 1ë‹¨ê³„ì—ì„œ ë§Œë“  browser_instanceë¥¼ VisitToolì— ë„˜ê²¨ì£¼ëŠ” ê²ƒì´ í•µì‹¬!\n",
    "browser_tools = [\n",
    "    GoogleSearchTool(provider=\"serper\"),\n",
    "    VisitTool(browser_instance),\n",
    "]\n",
    "\n",
    "print(\"âœ… ë¸Œë¼ìš°ì € ì œì–´ ë„êµ¬ë“¤ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[tool.name for tool in browser_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795ed5d",
   "metadata": {},
   "source": [
    "### ë‹¨ê³„ 3: Agent íŒ€ êµ¬ì„±í•˜ê¸° (Worker & Manager)\n",
    "open_deep_researchì˜ í•µì‹¬ì€ ì—­í• ì„ ë¶„ë‹´í•˜ëŠ” Agent íŒ€ì…ë‹ˆë‹¤.\n",
    "\n",
    "Worker (search_agent): ToolCallingAgentë¡œ, ìš°ë¦¬ê°€ ë§Œë“  ë¸Œë¼ìš°ì € ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì •ë³´ ìˆ˜ì§‘ë§Œ ì „ë¬¸ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "Manager (manager_agent): CodeAgentë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ ì „ì²´ ë¦¬ì„œì¹˜ ê³„íšì„ ì„¸ì›ë‹ˆë‹¤. Workerì—ê²Œ ì‘ì—…ì„ ì§€ì‹œí•˜ê³ , ë°›ì€ ê²°ê³¼ë¥¼ ë¶„ì„/ê°€ê³µí•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ë§Œë“­ë‹ˆë‹¤. Python ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆì–´ ë°ì´í„° ì²˜ë¦¬ë‚˜ ê³„ì‚°ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b901a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Worker Agent (search_agent) ìƒì„± ì™„ë£Œ!\n",
      "âœ… Manager Agent ìƒì„± ì™„ë£Œ! ì´ì œ ë¦¬ì„œì¹˜ë¥¼ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from smolagents import ToolCallingAgent, CodeAgent, LiteLLMModel\n",
    "\n",
    "# LLM ëª¨ë¸ ì •ì˜\n",
    "model = LiteLLMModel(model_id=\"gpt-4.1-mini\")\n",
    "\n",
    "# 1. Worker Agent (ToolCallingAgent) ìƒì„±\n",
    "search_agent = ToolCallingAgent(\n",
    "    model=model,\n",
    "    tools=browser_tools, # 2ë‹¨ê³„ì—ì„œ ë§Œë“  ë„êµ¬ ëª©ë¡ì„ ì „ë‹¬\n",
    "    name=\"search_agent\",\n",
    "    description=\"\"\"ì›¹ ê²€ìƒ‰ ë° í˜ì´ì§€ ë°©ë¬¸ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ë¦¬ì„œì²˜ì…ë‹ˆë‹¤. íŠ¹ì • ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì´ Agentì—ê²Œ ëª…í™•í•œ ì§€ì‹œë¥¼ ë‚´ë¦¬ì„¸ìš”.\"\"\",\n",
    "    verbosity_level=2, # Agentì˜ ì‘ì—… ê³¼ì •ì„ ìƒì„¸íˆ ë³´ë ¤ë©´ 2ë¡œ ì„¤ì •\n",
    ")\n",
    "\n",
    "print(\"âœ… Worker Agent (search_agent) ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# 2. Manager Agent (CodeAgent) ìƒì„±\n",
    "manager_agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    managed_agents=[search_agent], # ê´€ë¦¬í•  Worker Agentë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    verbosity_level=2\n",
    ")\n",
    "\n",
    "print(\"âœ… Manager Agent ìƒì„± ì™„ë£Œ! ì´ì œ ë¦¬ì„œì¹˜ë¥¼ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86110a",
   "metadata": {},
   "source": [
    "### ë‹¨ê³„ 4: Deep Research Agent ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸\n",
    "ì´ì œ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤. Manager Agentì—ê²Œ ë¦¬ì„œì¹˜ ì„ë¬´ë¥¼ ì£¼ê³ , Workerì™€ ì–´ë–»ê²Œ í˜‘ë ¥í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ ì§€ì¼œë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301feaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤” ì§ˆë¬¸: smol-agentì™€ LangChainì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì´ê³ , ê°ê° ì–´ë–¤ ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•œê°€ìš”?\n",
      "\n",
      "ğŸš€ Manager Agentê°€ ë¦¬ì„œì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">smol-agentì™€ LangChainì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì´ê³ , ê°ê° ì–´ë–¤ ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•œê°€ìš”?</span>                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ LiteLLMModel - gpt-4.1-mini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1msmol-agentì™€ LangChainì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì´ê³ , ê°ê° ì–´ë–¤ ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•œê°€ìš”?\u001b[0m                  \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - gpt-4.1-mini \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Output message of the LLM:</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: The user wants to know the main differences between smol-agent and LangChain and the situations where each</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">is more suitable. This is a comparative question about two software/tools, likely related to AI or language models.</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">I should search for reliable information or documentation about both smol-agent and LangChain to understand their </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">key features and use cases. I will ask the search_agent to gather information on the differences and suitable use </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">cases for both smol-agent and LangChain.</span><span style=\"background-color: #0d1117\">                                                                           </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">&lt;code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                             </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">task_description = (</span><span style=\"background-color: #0d1117\">                                                                                               </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"Please provide detailed information on the main differences between smol-agent and LangChain. \"</span><span style=\"background-color: #0d1117\">               </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"Additionally, explain in which scenarios or use cases each of these tools is more appropriate to use.\"</span><span style=\"background-color: #0d1117\">        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">)</span><span style=\"background-color: #0d1117\">                                                                                                                  </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">info = search_agent(task=task_description, additional_args={})</span><span style=\"background-color: #0d1117\">                                                     </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(info)</span><span style=\"background-color: #0d1117\">                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">&lt;/code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3mOutput message of the LLM:\u001b[0m \u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muser\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwants\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mknow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbetween\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msituations\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhere\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23meach\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuitable.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomparative\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquestion\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtwo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msoftware/tools,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlikely\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelated\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlanguage\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels.\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mshould\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreliable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mboth\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23munderstand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtheir\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mkey\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfeatures\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcases.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mask\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch_agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgather\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuitable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcases\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mboth\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain.\u001b[0m\u001b[48;2;13;17;23m                                                                           \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m<code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                             \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtask_description\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[48;2;13;17;23m                                                                                               \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"Please\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovide\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetailed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbetween\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m               \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"Additionally,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexplain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhich\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscenarios\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcases\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23meach\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthese\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtools\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mappropriate\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse.\"\u001b[0m\u001b[48;2;13;17;23m        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                  \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23minfo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch_agent(task=task_description,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madditional_args={})\u001b[0m\u001b[48;2;13;17;23m                                                     \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprint(info)\u001b[0m\u001b[48;2;13;17;23m                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m</code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing parsed code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">task_description </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> (</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Please provide detailed information on the main differences between smol-agent and LangChain. \"</span><span style=\"background-color: #272822\">           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Additionally, explain in which scenarios or use cases each of these tools is more appropriate to use.\"</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> search_agent(task</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">task_description, additional_args</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">{})</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(info)</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting parsed code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtask_description\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPlease provide detailed information on the main differences between smol-agent and LangChain. \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mAdditionally, explain in which scenarios or use cases each of these tools is more appropriate to use.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34minfo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_agent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtask\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtask_description\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madditional_args\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minfo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run - search_agent</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">You're a helpful agent named 'search_agent'.</span>                                                                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">You have been submitted this task by your manager.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">---</span>                                                                                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Task:</span>                                                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Please provide detailed information on the main differences between smol-agent and LangChain. Additionally, </span>    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">explain in which scenarios or use cases each of these tools is more appropriate to use.</span>                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">---</span>                                                                                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">information as possible to give them a clear understanding of the answer.</span>                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Your final_answer WILL HAVE to contain these parts:</span>                                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">### 1. Task outcome (short version):</span>                                                                            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">### 2. Task outcome (extremely detailed version):</span>                                                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">### 3. Additional context (if relevant):</span>                                                                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">lost.</span>                                                                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">And even if your task resolution is not successful, please return as much context as possible, so that your </span>    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">manager can act upon this feedback.</span>                                                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">You have been provided with these additional arguments, that you can access using the keys as variables in your</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">python code:</span>                                                                                                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">{}.</span>                                                                                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ LiteLLMModel - gpt-4.1-mini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run - search_agent\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mYou're a helpful agent named 'search_agent'.\u001b[0m                                                                    \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mYou have been submitted this task by your manager.\u001b[0m                                                              \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1m---\u001b[0m                                                                                                             \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mTask:\u001b[0m                                                                                                           \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mPlease provide detailed information on the main differences between smol-agent and LangChain. Additionally, \u001b[0m    \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mexplain in which scenarios or use cases each of these tools is more appropriate to use.\u001b[0m                         \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1m---\u001b[0m                                                                                                             \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mYou're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much\u001b[0m \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1minformation as possible to give them a clear understanding of the answer.\u001b[0m                                       \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mYour final_answer WILL HAVE to contain these parts:\u001b[0m                                                             \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1m### 1. Task outcome (short version):\u001b[0m                                                                            \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1m### 2. Task outcome (extremely detailed version):\u001b[0m                                                               \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1m### 3. Additional context (if relevant):\u001b[0m                                                                        \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mPut all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be\u001b[0m \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mlost.\u001b[0m                                                                                                           \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mAnd even if your task resolution is not successful, please return as much context as possible, so that your \u001b[0m    \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mmanager can act upon this feedback.\u001b[0m                                                                             \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mYou have been provided with these additional arguments, that you can access using the keys as variables in your\u001b[0m \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mpython code:\u001b[0m                                                                                                    \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1m{}.\u001b[0m                                                                                                             \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - gpt-4.1-mini \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Output message of the LLM:</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">ModelResponse(id='chatcmpl-BqFa2jRymyrCvnawBq2hKqSsreSq0', created=1751792646, model='gpt-4.1-mini-2025-04-14', </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">object='chat.completion', system_fingerprint='fp_6f2eabb9a5', choices=[Choices(finish_reason='tool_calls', index=0,</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">message=Message(content=None, role='assistant', </span><span style=\"background-color: #0d1117\">                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"query\": \"smol-agent vs LangChain </span><span style=\"background-color: #0d1117\">         </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">differences\"}', name='web_search'), id='call_ptIG068ydLyrKERmsLIxQYW0', type='function'), </span><span style=\"background-color: #0d1117\">                         </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">ChatCompletionMessageToolCall(function=Function(arguments='{\"query\": \"use cases for smol-agent and LangChain\"}', </span><span style=\"background-color: #0d1117\">  </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">name='web_search'), id='call_m8VpGNXVHMCmQweReD3AJmtY', type='function')], function_call=None, </span><span style=\"background-color: #0d1117\">                    </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], </span><span style=\"background-color: #0d1117\">                       </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">usage=Usage(completion_tokens=58, prompt_tokens=1286, total_tokens=1344, </span><span style=\"background-color: #0d1117\">                                          </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, </span><span style=\"background-color: #0d1117\">            </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), </span><span style=\"background-color: #0d1117\">                                              </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, </span><span style=\"background-color: #0d1117\">               </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">image_tokens=None)), service_tier='default')</span><span style=\"background-color: #0d1117\">                                                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3mOutput message of the LLM:\u001b[0m \u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mModelResponse(id='chatcmpl-BqFa2jRymyrCvnawBq2hKqSsreSq0',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcreated=1751792646,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel='gpt-4.1-mini-2025-04-14',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mobject='chat.completion',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msystem_fingerprint='fp_6f2eabb9a5',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchoices=[Choices(finish_reason='tool_calls',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mindex=0,\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mmessage=Message(content=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrole='assistant',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"query\":\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"smol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mvs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m         \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\"}',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mname='web_search'),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mid='call_ptIG068ydLyrKERmsLIxQYW0',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtype='function'),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                         \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mChatCompletionMessageToolCall(function=Function(arguments='{\"query\":\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"use\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcases\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\"}',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mname='web_search'),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mid='call_m8VpGNXVHMCmQweReD3AJmtY',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtype='function')],\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunction_call=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                    \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprovider_specific_fields={'refusal':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mNone},\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mannotations=[]),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovider_specific_fields={})],\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                       \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23musage=Usage(completion_tokens=58,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprompt_tokens=1286,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtotal_tokens=1344,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                          \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcompletion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23maudio_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m            \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mreasoning_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrejected_prediction_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtext_tokens=None),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                              \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcached_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtext_tokens=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m               \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mimage_tokens=None)),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mservice_tier='default')\u001b[0m\u001b[48;2;13;17;23m                                                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'web_search' with arguments: {'query': 'smol-agent vs LangChain differences'}                     â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'web_search' with arguments: {'query': 'smol-agent vs LangChain differences'}                     â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'web_search' with arguments: {'query': 'use cases for smol-agent and LangChain'}                  â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'web_search' with arguments: {'query': 'use cases for smol-agent and LangChain'}                  â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ## Search Results\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>. |LlamaIndex vs. LangChain vs. Hugging Face smolagent - Towards \n",
       "AI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pub.towardsai.net/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">a402)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Each framework offers a unique architectural vision, performance optimization strategy, and scalability approach â€” \n",
       "shining in different use <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. |Smolagents vs LangGraph: Which is Better? - Analytics \n",
       "Vidhya<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.analyticsvidhya.com/blog/2025/01/smolagents-vs-langgraph/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span> Jan <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "This article delves into the features and capabilities of both these models, providing a detailed comparison of \n",
       "smolagents vs LangGraph.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. |Comparing Smolagents vs. LangGraph: Which Agentic Framework \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hungvtm.medium.com/comparing-smolagents-vs-langgraph-which-agentic-framework-is-right-for-you-ad255dd6</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">a1cb)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "In this post, we dive into their core differences, use cases, overall ease of use, and the concept of Flow \n",
       "Engineering as an innovative way to <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. |Comprehensive Comparison of AI Agent Frameworks - \n",
       "Medium<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@mohitcharan04/comprehensive-comparison-of-ai-agent-frameworks-bec7d25df8a6)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Smolagents claims ~<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>% fewer steps and LLM calls vs. JSON-style tool usage, yielding better efficiency on complex \n",
       "benchmarks. Code agents: The <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. |LlamaIndex vs. LangChain vs. Hugging Face smolagent - \n",
       "Facebook<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.facebook.com/towardsAl/posts/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensiv</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">e-comparison-via/958084473137712/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "LlamaIndex vs. LangChain vs. Hugging Face smolagent: A Comprehensive Comparison via #TowardsAI â†’\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. |<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> AI Agent Frameworks You Should Know in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span> - Thinkstack \n",
       "AI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.thinkstack.ai/blog/best-ai-agent-frameworks/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> May <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. Smol Agents. While many AI frameworks aim for scale, Smol Agents takes a different path â€” simplicity, \n",
       "transparency, and lightweight control.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. |Comparing Open-Source AI Agent Frameworks - Langfuse \n",
       "Blog<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://langfuse.com/blog/2025-03-19-ai-agent-comparison)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Instead of juggling complex multi-step prompts or advanced orchestration, smolagents sets up a minimal loop where \n",
       "the agent writes and executes <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. |The Hugging Face Agents Course now includes three major <span style=\"color: #808000; text-decoration-color: #808000\">...</span> - \n",
       "Reddit<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "This includes LlamaIndex, LangChain, and our very own smolagents. We've worked to integrate the three frameworks in\n",
       "distinctive ways so that learners can <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. |Richard Seroter on X: \"LlamaIndex vs. LangChain vs. Hugging Face \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://x.com/rseroter/status/1901751396145168503)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "LlamaIndex vs. LangChain vs. Hugging Face smolagent: A Comprehensive Comparison | Towards AI. <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Author</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>: Can Demir \n",
       "Originally published on <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>. |- YouTube<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://m.youtube.com/post/UgkxNNXhSXaBR601OV8PsPP4WKM2pQuxeLEH)</span>\n",
       "\n",
       "And you asked, how is smolagents different from LangChain and how is it different from OpenAI's agents, especially \n",
       "the swarm code? I will answer your <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: ## Search Results\n",
       "\u001b[1;36m0\u001b[0m. |LlamaIndex vs. LangChain vs. Hugging Face smolagent - Towards \n",
       "AI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pub.towardsai.net/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1\u001b[0m\n",
       "\u001b[4;94ma402\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m10\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Each framework offers a unique architectural vision, performance optimization strategy, and scalability approach â€” \n",
       "shining in different use \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. |Smolagents vs LangGraph: Which is Better? - Analytics \n",
       "Vidhya\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.analyticsvidhya.com/blog/2025/01/smolagents-vs-langgraph/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m29\u001b[0m Jan \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "This article delves into the features and capabilities of both these models, providing a detailed comparison of \n",
       "smolagents vs LangGraph.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. |Comparing Smolagents vs. LangGraph: Which Agentic Framework \n",
       "\u001b[33m...\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hungvtm.medium.com/comparing-smolagents-vs-langgraph-which-agentic-framework-is-right-for-you-ad255dd6\u001b[0m\n",
       "\u001b[4;94ma1cb\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m23\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "In this post, we dive into their core differences, use cases, overall ease of use, and the concept of Flow \n",
       "Engineering as an innovative way to \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. |Comprehensive Comparison of AI Agent Frameworks - \n",
       "Medium\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@mohitcharan04/comprehensive-comparison-of-ai-agent-frameworks-bec7d25df8a6\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m17\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Smolagents claims ~\u001b[1;36m30\u001b[0m% fewer steps and LLM calls vs. JSON-style tool usage, yielding better efficiency on complex \n",
       "benchmarks. Code agents: The \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. |LlamaIndex vs. LangChain vs. Hugging Face smolagent - \n",
       "Facebook\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.facebook.com/towardsAl/posts/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensiv\u001b[0m\n",
       "\u001b[4;94me-comparison-via/958084473137712/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m11\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "LlamaIndex vs. LangChain vs. Hugging Face smolagent: A Comprehensive Comparison via #TowardsAI â†’\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. |\u001b[1;36m7\u001b[0m AI Agent Frameworks You Should Know in \u001b[1;36m2025\u001b[0m - Thinkstack \n",
       "AI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.thinkstack.ai/blog/best-ai-agent-frameworks/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m5\u001b[0m May \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. Smol Agents. While many AI frameworks aim for scale, Smol Agents takes a different path â€” simplicity, \n",
       "transparency, and lightweight control.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. |Comparing Open-Source AI Agent Frameworks - Langfuse \n",
       "Blog\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://langfuse.com/blog/2025-03-19-ai-agent-comparison\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m19\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Instead of juggling complex multi-step prompts or advanced orchestration, smolagents sets up a minimal loop where \n",
       "the agent writes and executes \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. |The Hugging Face Agents Course now includes three major \u001b[33m...\u001b[0m - \n",
       "Reddit\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m21\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "This includes LlamaIndex, LangChain, and our very own smolagents. We've worked to integrate the three frameworks in\n",
       "distinctive ways so that learners can \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m8\u001b[0m. |Richard Seroter on X: \"LlamaIndex vs. LangChain vs. Hugging Face \n",
       "\u001b[33m...\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://x.com/rseroter/status/1901751396145168503\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m17\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "LlamaIndex vs. LangChain vs. Hugging Face smolagent: A Comprehensive Comparison | Towards AI. \u001b[1;35mAuthor\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m: Can Demir \n",
       "Originally published on \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m9\u001b[0m. |- YouTube\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://m.youtube.com/post/UgkxNNXhSXaBR601OV8PsPP4WKM2pQuxeLEH\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "And you asked, how is smolagents different from LangChain and how is it different from OpenAI's agents, especially \n",
       "the swarm code? I will answer your \u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ## Search Results\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>. |Use cases for small models? : r/LangChain - \n",
       "Reddit<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.reddit.com/r/LangChain/comments/1gpy44x/use_cases_for_small_models/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> Nov <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>\n",
       "\n",
       "The biggest advantage of small models is they are super fast. However, the tradeoff is the ability to follow \n",
       "instructions and reasoning is really bad.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. |Harnessing HuggingFace's SmolAgent for Smart Web \n",
       "Automation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@sumit.somanchd/harnessing-huggingfaces-smolagent-for-smart-web-automation-e53e5204c</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">d4f)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> Jan <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Future Use Cases for Smolagents Â· Travel Planning: Agents that interact with APIs like Google Maps to plan travel \n",
       "itineraries based on real-time <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. |smolagents: a barebones library for agents that think in code. - \n",
       "GitHub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/smolagents)</span>\n",
       "\n",
       "smolagents is a library that enables you to run powerful agents in a few lines of code. It offers: âœ¨ Simplicity: \n",
       "the logic for agents fits in ~<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> lines <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. |Smolagents vs LangGraph: Which is Better? - Analytics \n",
       "Vidhya<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.analyticsvidhya.com/blog/2025/01/smolagents-vs-langgraph/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span> Jan <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Smolagents vs LangGraph: Key Features ; Use Cases, Designed for rapid prototyping and simple agents. Suitable for \n",
       "enterprise workflows and multi- <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. |Build an agent with tool-calling superpowers using smolagents<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/learn/cookbook/en/agents)</span>\n",
       "\n",
       "This notebook demonstrates how you can use smolagents to build awesome agents! <span style=\"color: #808000; text-decoration-color: #808000\">...</span> Let's see how to use it, and \n",
       "which use cases it can solve. Run the line <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. |LlamaIndex vs. LangChain vs. Hugging Face smolagent - Towards \n",
       "AI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://towardsai.net/p/machine-learning/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comp</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">arison)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> Mar <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "LangChain is a go-to solution for chatbots, question-answering systems, or any scenario that needs flexible chains \n",
       "of prompts, memory, and tool <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. |Tools of Smolagents in-depth guide<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://smolagents.org/docs/tools-of-smolagents-in-depth-guide/)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> Jan <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Here is how you can use it to recreate the intro's search result using a LangChain web search tool. This tool will \n",
       "need pip install langchain <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. |Top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> Use Cases of LangChain and LangGraph - \n",
       "LinkedIn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.linkedin.com/pulse/exploring-frontiers-ai-top-5-use-cases-langchain-dileep-kumar-pandiya-hos3</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">e)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> Dec <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>\n",
       "\n",
       "Example: A personal assistant application can use LangChain agents to manage tasks like scheduling appointments, \n",
       "sending emails, and booking <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. |Building LLM Agents with Smol Agents by Hugging Face <span style=\"color: #808000; text-decoration-color: #808000\">...</span> - \n",
       "Medium<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/algomart/smol-but-mighty-building-llm-agents-with-smol-agents-by-hugging-face-a862cd1f30</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">04)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> Apr <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "Smol Agents is about one thing: letting large language models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> call Python functions directly â€” without \n",
       "needing pages of scaffolding code <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>. |Building Multi AI Agent Workflows With LangChain In <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span> - \n",
       "Intuz<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.intuz.com/blog/building-multi-ai-agent-workflows-with-langchain)</span>\n",
       "Date published: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> May <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "With LangChain, even small and medium businesses can now build smart, scalable AI workflows where multiple agents \n",
       "collaborate to automate complex tasks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: ## Search Results\n",
       "\u001b[1;36m0\u001b[0m. |Use cases for small models? : r/LangChain - \n",
       "Reddit\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.reddit.com/r/LangChain/comments/1gpy44x/use_cases_for_small_models/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m12\u001b[0m Nov \u001b[1;36m2024\u001b[0m\n",
       "\n",
       "The biggest advantage of small models is they are super fast. However, the tradeoff is the ability to follow \n",
       "instructions and reasoning is really bad.\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. |Harnessing HuggingFace's SmolAgent for Smart Web \n",
       "Automation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@sumit.somanchd/harnessing-huggingfaces-smolagent-for-smart-web-automation-e53e5204c\u001b[0m\n",
       "\u001b[4;94md4f\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m25\u001b[0m Jan \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Future Use Cases for Smolagents Â· Travel Planning: Agents that interact with APIs like Google Maps to plan travel \n",
       "itineraries based on real-time \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. |smolagents: a barebones library for agents that think in code. - \n",
       "GitHub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/smolagents\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "smolagents is a library that enables you to run powerful agents in a few lines of code. It offers: âœ¨ Simplicity: \n",
       "the logic for agents fits in ~\u001b[1;36m1\u001b[0m,\u001b[1;36m000\u001b[0m lines \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. |Smolagents vs LangGraph: Which is Better? - Analytics \n",
       "Vidhya\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.analyticsvidhya.com/blog/2025/01/smolagents-vs-langgraph/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m29\u001b[0m Jan \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Smolagents vs LangGraph: Key Features ; Use Cases, Designed for rapid prototyping and simple agents. Suitable for \n",
       "enterprise workflows and multi- \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. |Build an agent with tool-calling superpowers using smolagents\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/learn/cookbook/en/agents\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "This notebook demonstrates how you can use smolagents to build awesome agents! \u001b[33m...\u001b[0m Let's see how to use it, and \n",
       "which use cases it can solve. Run the line \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. |LlamaIndex vs. LangChain vs. Hugging Face smolagent - Towards \n",
       "AI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://towardsai.net/p/machine-learning/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comp\u001b[0m\n",
       "\u001b[4;94marison\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m10\u001b[0m Mar \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "LangChain is a go-to solution for chatbots, question-answering systems, or any scenario that needs flexible chains \n",
       "of prompts, memory, and tool \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. |Tools of Smolagents in-depth guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://smolagents.org/docs/tools-of-smolagents-in-depth-guide/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m11\u001b[0m Jan \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Here is how you can use it to recreate the intro's search result using a LangChain web search tool. This tool will \n",
       "need pip install langchain \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. |Top \u001b[1;36m5\u001b[0m Use Cases of LangChain and LangGraph - \n",
       "LinkedIn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.linkedin.com/pulse/exploring-frontiers-ai-top-5-use-cases-langchain-dileep-kumar-pandiya-hos3\u001b[0m\n",
       "\u001b[4;94me\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m20\u001b[0m Dec \u001b[1;36m2024\u001b[0m\n",
       "\n",
       "Example: A personal assistant application can use LangChain agents to manage tasks like scheduling appointments, \n",
       "sending emails, and booking \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m8\u001b[0m. |Building LLM Agents with Smol Agents by Hugging Face \u001b[33m...\u001b[0m - \n",
       "Medium\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/algomart/smol-but-mighty-building-llm-agents-with-smol-agents-by-hugging-face-a862cd1f30\u001b[0m\n",
       "\u001b[4;94m04\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m7\u001b[0m Apr \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "Smol Agents is about one thing: letting large language models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m call Python functions directly â€” without \n",
       "needing pages of scaffolding code \u001b[33m...\u001b[0m\n",
       "\n",
       "\u001b[1;36m9\u001b[0m. |Building Multi AI Agent Workflows With LangChain In \u001b[1;36m2025\u001b[0m - \n",
       "Intuz\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.intuz.com/blog/building-multi-ai-agent-workflows-with-langchain\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "Date published: \u001b[1;36m27\u001b[0m May \u001b[1;36m2025\u001b[0m\n",
       "\n",
       "With LangChain, even small and medium businesses can now build smart, scalable AI workflows where multiple agents \n",
       "collaborate to automate complex tasks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.67 seconds| Input tokens: 1,286 | Output tokens: 58]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.67 seconds| Input tokens: 1,286 | Output tokens: 58]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Output message of the LLM:</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">ModelResponse(id='chatcmpl-BqFa6G3HTEbW8DKUAqI8TSx2370Ez', created=1751792650, model='gpt-4.1-mini-2025-04-14', </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">object='chat.completion', system_fingerprint='fp_6f2eabb9a5', choices=[Choices(finish_reason='tool_calls', index=0,</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">message=Message(content=None, role='assistant', </span><span style=\"background-color: #0d1117\">                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"url\":\"https://pub.towardsai.net/llamaindex</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402\"}', name='visit_page'), </span><span style=\"background-color: #0d1117\">           </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">id='call_BwB4DVBkkRenlkRM8Jr0LWzN', type='function')], function_call=None, provider_specific_fields={'refusal': </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=53, prompt_tokens=3116, </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">total_tokens=3169, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, </span><span style=\"background-color: #0d1117\">         </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), </span><span style=\"background-color: #0d1117\">                              </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=1280, text_tokens=None, </span><span style=\"background-color: #0d1117\">            </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">image_tokens=None)), service_tier='default')</span><span style=\"background-color: #0d1117\">                                                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3mOutput message of the LLM:\u001b[0m \u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mModelResponse(id='chatcmpl-BqFa6G3HTEbW8DKUAqI8TSx2370Ez',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcreated=1751792650,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel='gpt-4.1-mini-2025-04-14',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mobject='chat.completion',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msystem_fingerprint='fp_6f2eabb9a5',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchoices=[Choices(finish_reason='tool_calls',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mindex=0,\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mmessage=Message(content=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrole='assistant',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"url\":\"https://pub.towardsai.net/llamaindex\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402\"}',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mname='visit_page'),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m           \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mid='call_BwB4DVBkkRenlkRM8Jr0LWzN',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtype='function')],\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunction_call=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovider_specific_fields={'refusal':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mNone},\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mannotations=[]),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovider_specific_fields={})],\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23musage=Usage(completion_tokens=53,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprompt_tokens=3116,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtotal_tokens=3169,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompletion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m         \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23maudio_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrejected_prediction_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtext_tokens=None),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                              \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcached_tokens=1280,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtext_tokens=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m            \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mimage_tokens=None)),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mservice_tier='default')\u001b[0m\u001b[48;2;13;17;23m                                                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'visit_page' with arguments: {'url':                                                              â”‚\n",
       "â”‚ 'https://pub.towardsai.net/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b â”‚\n",
       "â”‚ 1a402'}                                                                                                         â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'visit_page' with arguments: {'url':                                                              â”‚\n",
       "â”‚ 'https://pub.towardsai.net/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b â”‚\n",
       "â”‚ 1a402'}                                                                                                         â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‰ Browser navigating to: https://pub.towardsai.net/llamaindex-vs-langchain-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: |Sitemap<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/sitemap/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sitemap.xml</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "|Open in \n",
       "app<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e9d86b1a402&amp;%7Efeature=LoOpenInAppButt</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">on&amp;%7Echannel=ShowPostUnderCollection&amp;%7Estage=mobileNavBar&amp;source=post_page---top_nav_layout_nav------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------------------)</span>\n",
       "\n",
       "|Sign \n",
       "up<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langcha</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">in-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&amp;source=post_page---top_nav_layout_nav---------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">--------------global_nav------------------)</span>\n",
       "\n",
       "|Sign \n",
       "in<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&amp;source=post_page---top_nav_layout_nav------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------global_nav------------------)</span>\n",
       "\n",
       "|Medium Logo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)</span>\n",
       "\n",
       "|Write<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---top_</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">nav_layout_nav-----------------------new_post_topnav------------------)</span>\n",
       "\n",
       "|Sign \n",
       "up<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langcha</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">in-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&amp;source=post_page---top_nav_layout_nav---------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">--------------global_nav------------------)</span>\n",
       "\n",
       "|Sign \n",
       "in<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&amp;source=post_page---top_nav_layout_nav------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------global_nav------------------)</span>\n",
       "\n",
       "!|<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:64:64/1</span>*dmbNkD5D-u45r44go_cf0g.png<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "|Towards AI\n",
       "----------<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-1e9d86b1a402----------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------------------)</span>\n",
       "\n",
       "Â·\n",
       "\n",
       "|Follow \n",
       "publication<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">artificial-intelligence&amp;operation=register&amp;redirect=https%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-vs-hu</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">gging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&amp;collection=Towards+AI&amp;collectionId=98111c9905da&amp;source</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">=post_page---publication_nav-98111c9905da-1e9d86b1a402---------------------publication_nav------------------)</span>\n",
       "\n",
       "|!|Towards \n",
       "AI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:76:76/1</span>*JyIThO-cLjlChQLb6kSlVQ.png<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pub.towardsai.net/?source=p</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ost_page---post_publication_sidebar-98111c9905da-1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "The leading AI community and content platform focused on making AI accessible to all. Check out our new course \n",
       "platform: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Follow </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">publication</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">artificial-intelligence&amp;operation=register&amp;redirect=https%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-vs-hu</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">gging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&amp;collection=Towards+AI&amp;collectionId=98111c9905da&amp;source</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">=post_page---post_publication_sidebar-98111c9905da-1e9d86b1a402---------------------post_publication_sidebar-------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex vs. LangChain vs. Hugging Face smolagent: A Comprehensive Comparison</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">===============================================================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|!|Can </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Demir</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:64:64/1</span><span style=\"color: #000000; text-decoration-color: #000000\">*w-QZ4D_8jxIZ-0iZVQiVpw.jpeg</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir13?</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">source=post_page---byline--1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Can </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Demir</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir13?source=post_page---byline--1e9d86b1a402--------------------------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #000000; text-decoration-color: #000000\"> min read</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Â·</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Mar </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Listen</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Share</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!|</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]()</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Introduction</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">============</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Large Language Models </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">LLMs</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> have opened up a new world of possibilities, powering everything from advanced </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">chatbots to autonomous AI agents. However, to unlock their full potential, you often need robust frameworks that </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">handle data ingestion, prompt engineering, memory storage, and tool usage. Three significant solutions have emerged</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">in this space: **LlamaIndex**, **LangChain**, and Hugging Faceâ€™s **smolagent** approach.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Each framework offers a unique architectural vision, performance optimization strategy, and scalability approach â€” </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">shining in different use cases. In this article, weâ€™ll take a deep dive into all three, comparing:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Their design philosophies,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Real-world use cases such as information retrieval and agent development,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Pros and cons,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Guidance on choosing the one that best aligns with your project goals.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This tutorial-style guide aims to deliver practical insights beyond the official documentation, helping you make an</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">informed choice for your next LLM-powered application.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">A Quick Overview of the Frameworks</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">GPT Index</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">======================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Core Focus**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   LlamaIndex </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">formerly GPT Index</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> specializes in efficiently connecting LLMs to external data. Its power lies in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">data **indexing** and **retrieval**, allowing you to quickly query large datasets.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **How It Works**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   You feed your documents </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">files, databases, APIs, etc.</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> into LlamaIndex to build various index structures </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">vector</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">similarity, keyword tables, knowledge graphs, etc.</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. When a query arrives, LlamaIndex finds and returns only the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">relevant chunks to the LLM.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Strengths**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   It excels in **retrieval-augmented generation </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">RAG</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">** scenarios, where the model requires external context to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">generate accurate answers. Itâ€™s built for enterprise-scale data </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">potentially millions of documents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> without </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">sacrificing performance.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Specialization**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   LlamaIndex functions as the â€œknowledgeâ€ engine of an LLM application â€” particularly for data-heavy setups. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Although itâ€™s expanding into agent and tool functionality, its main value remains highly efficient access to large </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">corpora.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=========</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Core Focus**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   LangChain provides a broad, **modular** framework for building LLM-driven applications. Its hallmark is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">composability: prompt templates, memory modules, tool usage, chain-of-thought sequences, and more.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **How It Works**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   You assemble â€œchainsâ€ of LLM interactions. For example, you might feed user input into a retrieval module, then </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pass the retrieved context plus user query to the LLM. Add memory for conversation context, or define agents that </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">can decide which external tools to call in real time.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Strengths**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Known as the â€œglueâ€ connecting LLMs with various data sources and APIs, LangChain shines at **multi-step </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reasoning** and orchestrating complex workflows. Its vast community and ecosystem mean you can plug in almost any </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">vector database, LLM provider, or custom tool.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Specialization**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   LangChain is a go-to solution for chatbots, question-answering systems, or any scenario that needs flexible </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">chains of prompts, memory, and tool usage. It aims to cover â€œall things LLM,â€ from simple prototypes to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">sophisticated production-grade workflows.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">======================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Core Focus**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Hugging Faceâ€™s **smolagent** approach </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">initially introduced as Code Agents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> puts a fresh spin on AI agents by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">having the LLM generate **literal Python code** to solve tasks. Instead of returning a text solution, the model can</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">write and execute code that uses tools.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **How It Works**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   The agent might generate a snippet of Python </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">for instance, calling a search function, doing math, or parsing </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">data</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. This code is executed in a sandboxed environment, and the model uses any results for further reasoning. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tools are simply Python functions/classes that the agent can call.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Strengths**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   This approach is powerful for **multi-step tasks** requiring logic and computation. The agentâ€™s reasoning is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">transparent â€” you can read the code it wrote. It also integrates smoothly with the vast Hugging Face ecosystem of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">models, datasets, and pipelines.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Specialization**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   smolagent is excellent for open-source enthusiasts who want to avoid proprietary services and prefer the clarity</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">of code-based reasoning. Itâ€™s still experimental, so expect rapid evolution and a smaller </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">though growing</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">community.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Architecture and Design Philosophy</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==========</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Index + Query Engine**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   LlamaIndex revolves around building specialized indexes for large documents, then exposing a query engine to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">quickly retrieve relevant chunks. By constructing advanced data structures, LlamaIndex prevents the LLM from having</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to sift through massive text each time.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Data-Centric**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Itâ€™s all about â€œbringing your data to the LLM efficiently.â€ Indices can be vector-based for semantic search, or </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">they might rely on keyword matching, knowledge graphs, and so on.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Expanding Into Agents**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Recent releases add some agent-like features, but the primary value remains facilitating **scalable retrieval** </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">for LLMs in data-heavy applications.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=========</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Modular Building Blocks**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   LangChain defines interfaces for LLMs, prompts, memory modules, tools, output parsers, etc. You then build </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">â€œchainsâ€ that orchestrate calls across these components.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Chains and Agents**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   A chain is a straightforward linear flow. An agent is an LLM that decides which tool </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">if any</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to use at each </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">step </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">often following the ReAct paradigm</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Extensive Ecosystem**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Because itâ€™s so modular, LangChain lets you easily swap out an LLM or vector database. This flexibility can be </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">powerful â€” but thereâ€™s also a learning curve to master all the abstractions.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">======================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Code Generation Loop**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Here, the agent is literally writing Python code that calls various tools. Each tool is a simple Python </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">function, like `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">search</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">query</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">` or `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_image</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">prompt</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">`.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Planning = Execution**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   The LLMâ€™s plan to solve a task directly becomes the code it writes. You can observe and debug this code, which </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">is a unique advantage over purely prompt-based frameworks.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Experimental Status**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   While it offers strong potential </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">especially for advanced reasoning tasks</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, itâ€™s still early-stage. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Documentation, community support, and built-in features for memory or error handling are evolving.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!|</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]()</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Performance and Scalability</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">===========================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain Performance</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=====================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Dependent on Components**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Latency and throughput typically hinge on which LLM and data store you use, though LangChain helps with caching,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">batching, and asynchronous flows.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Horizontal Scaling**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   You can spin up multiple instances of a LangChain application and distribute requests. Each chain is relatively </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">stateless unless you explicitly maintain memory.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Complexity Cost**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   The more steps and calls in your chain, the higher the latency. Optimizing your chain to use only necessary </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">steps is key.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex Performance</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">======================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Optimized for Data Retrieval**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   By building indices upfront, LlamaIndex drastically cuts down the amount of text the LLM needs to process at </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">query time.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Scales with Data**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Indexing might be resource-intensive, but once done, queries remain fast even with millions of documents â€” </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">perfect for large-scale knowledge bases.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Incremental Updates**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   You can update indices periodically or in real time. For data-centric use cases, LlamaIndex often outperforms a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">naive approach, especially as data volumes grow.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent Performance</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Model + Code Execution**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   Performance depends on the selected LLMâ€™s inference speed and how complex the generated code is. Using an </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">open-source code model locally can be resource-heavy.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Multi-Step Overhead**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   A React-style agent may produce multiple code snippets, each adding a new LLM call and execution time. However, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">offloading computations to Python might sometimes be faster than the LLM struggling with large mental math in a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">single prompt.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **Scaling**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   The Hugging Face ecosystem supports containerization, on-prem, or cloud deployments. Caching and optimization </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">strategies for code-based agents are still developing.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!|</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]()</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Real-World Implementation Examples</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">: Information Retrieval </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">RAG for Q&amp;A</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==============================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Scenario**: You have a large document corpus and want a question-answering system that can fetch relevant </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">information from it.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex for Document Retrieval and Q&amp;A</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">-----------------------------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Designed for precisely this. A minimal code snippet:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">documents = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SimpleDirectoryReader</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"knowledge_docs\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.load_data</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">index = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTVectorStoreIndex.from_documents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">documents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">query_engine = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">index.as_query_engine</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">response = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">query_engine.query</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"What is the capital of the largest country in Europe by area?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">response.response</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* You get an answer grounded in the indexed docs, with minimal setup.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Perfect for large-scale retrieval-augmented generation.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain for Document Retrieval and Q&amp;A</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">----------------------------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Accomplishes the same but requires manually wiring components, for instance:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.embeddings import OpenAIEmbeddings  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.vectorstores import FAISS  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain import OpenAI  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.chains import RetrievalQA  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">embeddings = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAIEmbeddings</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">vector_db = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FAISS.from_texts</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">doc_texts, </span><span style=\"color: #808000; text-decoration-color: #808000\">embedding</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">embeddings</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">llm = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAI</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">temperature</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">qa_chain = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RetrievalQA.from_chain_type</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">llm, </span><span style=\"color: #808000; text-decoration-color: #808000\">chain_type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"stuff\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">retriever</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">vector_db</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.as_retriever</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">())</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">result = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">qa_chain.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"What is the capital of the largest country in Europe by area?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">result</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* You explicitly choose embeddings and a vector store.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* LangChainâ€™s flexibility is a plus, but thereâ€™s slightly more setup compared to LlamaIndexâ€™s straightforward </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">interface.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent for Retrieval</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">------------------------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">smolagent primarily focuses on code-based tool usage. A trivial example using a web search tool:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">agent = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CodeAgent</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tools</span><span style=\"color: #000000; text-decoration-color: #000000\">=|</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DuckDuckGoSearchTool</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">model</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HfApiModel</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">())</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">question = </span><span style=\"color: #008000; text-decoration-color: #008000\">\"What is the capital of the largest country in Europe by area?\"</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">answer = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">agent.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">question</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">answer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* The agent might generate Python code to search the web, parse results, and return the capital.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* If you have a private corpus, youâ€™d need a custom tool </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">e.g., `LocalDocSearchTool`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> rather than a web-based </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">search.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* For simple Q&amp;A, this can be overkill. It shines in multi-step tasks where code-based reasoning is advantageous.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">: Chatbot Development </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">Conversational Agents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">======================================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Scenario**: You need a conversational agent that retains context across user turns.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain for Chatbots</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">----------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain has built-in memory modules for context management:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.chat_models import ChatOpenAI  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.chains import ConversationChain  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.memory import ConversationBufferMemory  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">memory = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConversationBufferMemory</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">chat_chain = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConversationChain</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">llm</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOpenAI</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">temperature</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">memory</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">memory</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat_chain.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Hello, who are you?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">))</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat_chain.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Can you summarize what we've discussed so far?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">))</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* `ConversationChain` auto-injects previous turns into the prompt for a seamless chat experience.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* You can combine memory with retrieval, tool usage, and more.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex for Chatbots</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">-----------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex doesnâ€™t provide a full-fledged conversation flow manager out of the box. It focuses on retrieving </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">context from documents. You could:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Use LlamaIndex to fetch relevant data each turn, then pass it to your LLM prompt.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Store or summarize conversation history manually.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you only need short Q&amp;A on a knowledge base, LlamaIndex works fine. But for a free-form chatbot with multi-turn </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">memory, you usually pair it with a conversation framework </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">like LangChain</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent for Chatbots</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">-----------------------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">smolagent is not primarily geared toward extended dialogues with built-in memory. You can implement memory by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">feeding previous turns back into the agentâ€™s prompt each time:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">prompt = </span><span style=\"color: #008000; text-decoration-color: #008000\">\"User: Hello, how are you?\\nAssistant:\"</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">response = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">agent.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">prompt</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">But youâ€™ll have to track chat history yourself. The real advantage is if you want a chatbot that can **execute </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Python code** or call specialized tools mid-conversation. For a purely conversational use case, a simpler Chat LLM </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">or LangChainâ€™s memory features might be more convenient.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">: AI-Powered Agents and Tool Use</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=========================================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Scenario**: You want your LLM to not just chat but also take actions â€” calling APIs, running computations, etc.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain Agents </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">ReAct</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">------------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChainâ€™s Agents let an LLM reason step-by-step, calling tools as needed:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.llms import OpenAI  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from langchain.agents import load_tools, initialize_agent, AgentType  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">llm = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAI</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">temperature</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tools = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_tools</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">|</span><span style=\"color: #008000; text-decoration-color: #008000\">\"serpapi\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"llm-math\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">llm</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">llm</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">agent = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">initialize_agent</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">tools, llm, </span><span style=\"color: #808000; text-decoration-color: #808000\">agent</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">AgentType</span><span style=\"color: #000000; text-decoration-color: #000000\">.ZERO_SHOT_REACT_DESCRIPTION, </span><span style=\"color: #808000; text-decoration-color: #808000\">verbose</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">agent.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Who is the President of France, and what is his age multiplied by 2?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* The LLM decides: â€œFirst, letâ€™s use `serpapi` to find who the French president is. Next, weâ€™ll use `llm-math` to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">multiply his age by </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">.â€</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* This is great for multi-step reasoning with any set of tools you define.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex Within Agents</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">------------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You can integrate LlamaIndex as a â€œtoolâ€ inside a LangChain agent. For example:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* A â€œConsultDocsâ€ tool that internally calls `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">index.query</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">`.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* When the agent needs info from your knowledge base, it uses that tool.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex can serve as the retrieval powerhouse for an agent built in another framework.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">----------------------</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The **code-generation** approach is especially powerful for complex tasks:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">agent = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CodeAgent</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tools</span><span style=\"color: #000000; text-decoration-color: #000000\">=|</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DuckDuckGoSearchTool</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">model</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HfApiModel</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">())</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">query = </span><span style=\"color: #008000; text-decoration-color: #008000\">\"How many seconds would it take a leopard running at top speed to cross the Golden Gate Bridge?\"</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">result = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">agent.run</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">query</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">  </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">result</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* The LLM might generate Python code: searching the bridge length, top leopard speed, and then computing time = </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">distance </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> speed.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* It executes that code in a sandbox. You can inspect the generated code for debugging.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This is particularly helpful if the agent needs to do data parsing, multiple calculations, or chain various library</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">calls. Itâ€™s more transparent and sometimes more accurate than standard text-based ReAct.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Pros and Cons of Each Framework</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">===============================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LlamaIndex</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==========</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Pros**</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Excellent for Large-Scale Retrieval </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">RAG</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    If you have a massive corpus and want fast, accurate answers, LlamaIndex is a top choice.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Simple API**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    A few lines of code can index documents and start answering queries.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Scales Gracefully**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Pre-builds indices for quick queries even over millions of documents.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Interoperable**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Works with any LLM backend and can be integrated into broader agent frameworks </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">e.g., LangChain</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Rapid Feature Growth**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Constantly adding new index types and advanced querying options.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Cons**</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Narrower Focus**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Not a full conversation or agent framework â€” best for retrieval tasks.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Advanced Use Complexity**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Tuning indexes or customizing queries can require deeper knowledge.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Smaller Ecosystem**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Though growing, its community lags behind LangChainâ€™s in sheer size.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. **May Be Overkill for Small Data**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    If you only have a few pages of text, you might not need the overhead of building indices.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LangChain</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=========</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Pros**</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Highly Flexible and Modular**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    You can build nearly any LLM-driven workflow with its chain/agent/memory structure.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Rich Integrations**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Dozens of built-in connectors for vector stores, APIs, and LLM providers.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Easy Prototyping**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Many ready-to-use examples for chatbots, QA, translations, etc.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Built-In Memory and Prompt Handling**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Streamlines conversation design and advanced prompting.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Large Community**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Active forums, Discord, and third-party tutorials. Youâ€™re rarely alone in troubleshooting.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Cons**</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Can Be Overkill**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    For a single LLM call, LangChain might add unnecessary layers.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Steep Learning Curve**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Fully leveraging chains, agents, memory, and tools can be complex.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Runtime Overhead**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Each chain step or agent action adds latency and can complicate debugging.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Fast-Moving**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Frequent updates sometimes break APIs, requiring version pinning to maintain stability.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face smolagent</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">======================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Pros**</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Powerful, Code-Based Tool Use**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Ideal for multi-step tasks needing calculations, data manipulation, or advanced APIs.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Transparency and Debuggability**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    You can read the Python code the model writes, making error analysis easier.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Leverages Hugging Face Ecosystem**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Integrate any HF model or pipeline, plus open-source flexibility.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. **No Vendor Lock-In**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Fully open-source. You can self-host everything if you prefer.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Code-Centric Approach**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Offloading complexity to Python can boost accuracy for tasks like math or structured data handling.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Cons**</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Experimental**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    The API is still evolving, and the community is smaller than LangChainâ€™s.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Performance Overheads**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Generating and executing code can introduce additional latency, especially in multi-step loops.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Setup Complexity**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Sandboxing code, defining tools well, and preventing malicious/unsafe code requires extra care.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Not Primarily Conversation-Focused**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Out-of-the-box memory for long dialogues doesnâ€™t exist; you must build it yourself.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\">. **Uncertain Failure Modes**  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    If the model generates incorrect or buggy code, you need error-handling strategies </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">like retries or </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">self-correction</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Choosing the Right Framework</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">============================</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">How do you pick between these three? Here are some guidelines:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**â€œI have a huge corpus and need a Q&amp;A system over it.â€**</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* ***LlamaIndex*** *is your go-to. Itâ€™s purpose-built for speedy retrieval in large-scale data scenarios.*</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**â€œI want a general chatbot/tool application with multiple LLM interactions and memory.â€**</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* ***LangChain*** *provides modular building blocks for all sorts of LLM chains and agents, plus community </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">support.*</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**â€œMy AI needs to perform complex, multi-step tasks with open-source flexibility.â€**</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* ***Hugging Face smolagent****. If code-based logic is appealing </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">e.g., advanced calculations, dynamic Python </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">usage</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, youâ€™ll love smolagent.*</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**â€œI need something quick with minimal coding.â€**</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* ***LangChain*** *or* ***LlamaIndex****. For a simple Q&amp;A prototype, both are straightforward. Pick LlamaIndex if </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">data is huge; go with LangChain if you also need robust conversation or tool usage.*</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**â€œProduction-grade stability with strong support.â€**</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* ***LangChain*** *currently boasts the largest ecosystem and community;* ***LlamaIndex*** *is stable for retrieval</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tasks.* ***smolagent*** *is still maturing, so it might require more engineering effort for production.*</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Ultimately, these frameworks can be complementary. Many teams, for instance, combine LlamaIndex </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">for data </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">retrieval</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and LangChain </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">for conversation/agents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. You could even incorporate smolagent for code-based logic in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">specialized tasks. Usually, though, youâ€™ll choose one as the primary backbone and then pull in features from the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">others if needed.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Conclusion</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">==========</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The LLM application development landscape is advancing rapidly, with **LlamaIndex**, **LangChain**, and **Hugging </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Face smolagent** offering three compelling approaches. Weâ€™ve covered:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* How **LlamaIndex** excels at fast and scalable data retrieval,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* How **LangChain** provides a flexible, chain-of-thought framework for everything from simple chat to multi-step </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">agent orchestration,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* How **smolagent**â€™s code-generation paradigm can handle advanced logic and tool usage with remarkable </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">transparency.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Each framework has its own strengths and trade-offs â€” what matters is matching them to your projectâ€™s exact </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">requirements. The good news? All three are open source and easy to try. If youâ€™re unsure, spin up a quick prototype</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">in each, see which one feels most natural and performs best for your needs.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In this era of rapid innovation, frameworks may come and go, but the core principle remains: identify your </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">problemâ€™s priorities </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">data scale, complexity, conversation vs. tool usage</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and pick the framework that aligns with </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">those needs. LlamaIndex, LangChain, and smolagent are each brilliant in their own domain. Armed with the insights </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from this comparison, youâ€™ll be well-equipped to make your next LLM project a success.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Happy building!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Llamaindex</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/tag/llamaindex?source=post_page-----1e9d86b1a402-----------------------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">----)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Langchain </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Agents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/tag/langchain-agents?source=post_page-----1e9d86b1a402----------------------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Smolagents</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/tag/smolagents?source=post_page-----1e9d86b1a402-----------------------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">----)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Agentic </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Ai</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/tag/agentic-ai?source=post_page-----1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Llm </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Applications</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/tag/llm-applications?source=post_page-----1e9d86b1a402----------------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|!|Towards </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">AI</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:96:96/1</span><span style=\"color: #000000; text-decoration-color: #000000\">*JyIThO-cLjlChQLb6kSlVQ.png</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pub.towardsai.net/?source=p</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ost_page---post_publication_info--1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|!|Towards </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">AI</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:128:128/1</span><span style=\"color: #000000; text-decoration-color: #000000\">*JyIThO-cLjlChQLb6kSlVQ.png</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pub.towardsai.net/?source</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">=post_page---post_publication_info--1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Follow</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Published in Towards AI</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">-----------------------</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pub.towardsai.net/?source=post_page---post_publication_info--1e9d86b1a402---------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">------------------------------)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|83K </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">followers</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">followers</span><span style=\"color: #000000; text-decoration-color: #000000\">?</span><span style=\"color: #808000; text-decoration-color: #808000\">source</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">post_page</span><span style=\"color: #000000; text-decoration-color: #000000\">---post_publication_info--1e9d86b1a402---------------------------------------</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Â·|Last publishedÂ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000\"> hours </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">ago</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">how-to-really-build-on-top-of-llms-07822726c228</span><span style=\"color: #000000; text-decoration-color: #000000\">?</span><span style=\"color: #808000; text-decoration-color: #808000\">source</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">post_page</span><span style=\"color: #000000; text-decoration-color: #000000\">---post_publication_info--1e9d86b1a402-------</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--------------------------------</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The leading AI community and content platform focused on making AI accessible to all. Check out our new course </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">platform: &lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Follow\n",
       "\n",
       "|!|Can \n",
       "Demir<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:96:96/1</span>*w-QZ4D_8jxIZ-0iZVQiVpw.jpeg<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir13?</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">source=post_page---post_author_info--1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "|!|Can \n",
       "Demir<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://miro.medium.com/v2/resize:fill:128:128/1</span>*w-QZ4D_8jxIZ-0iZVQiVpw.jpeg<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir1</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">3?source=post_page---post_author_info--1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "|Written by Can Demir\n",
       "--------------------<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir13?source=post_page---post_author_info--1e9d86b1a402-------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">--------------------------)</span>\n",
       "\n",
       "|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span> \n",
       "followers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir13/followers?source=post_page---post_author_info--1e9d86b1a402--------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-------------------------)</span>\n",
       "\n",
       "Â·|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span> \n",
       "following<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/@candemir13/following?source=post_page---post_author_info--1e9d86b1a402--------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-------------------------)</span>\n",
       "\n",
       "AI/ML Engineer &amp; Data Professional with finance background. Developing enterprise-scale data &amp; AI solutions. \n",
       "Exploring tech innovations ğŸ“ˆ\n",
       "\n",
       "Responses <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------\n",
       "\n",
       "See all responses\n",
       "\n",
       "|Help<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://help.medium.com/hc/en-us?source=post_page-----1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "|Status<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.statuspage.io/?source=post_page-----1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "|About<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/about?autoplay=1&amp;source=post_page-----1e9d86b1a402--------------------------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-)</span>\n",
       "\n",
       "|Careers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1e9d86b1a402----------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------------------------)</span>\n",
       "\n",
       "|Press<span style=\"font-weight: bold\">](</span>mailto:pressinquiries@medium.com<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "|Blog<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://blog.medium.com/?source=post_page-----1e9d86b1a402---------------------------------------)</span>\n",
       "\n",
       "|Privacy<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e9d86b1a402-----------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">----------------------------)</span>\n",
       "\n",
       "|Rules<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----1e9d86b1a402----------------------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-----------------)</span>\n",
       "\n",
       "|Terms<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e9d86b1a402-----------</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">----------------------------)</span>\n",
       "\n",
       "|Text to \n",
       "speech<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://speechify.com/medium?source=post_page-----1e9d86b1a402---------------------------------------)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: |Sitemap\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/sitemap/\u001b[0m\u001b[95msitemap.xml\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "|Open in \n",
       "app\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://rsci.app.link/?%\u001b[0m\u001b[4;94m24canonical_url\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fmedium.com%2Fp%2F1e9d86b1a402&%\u001b[0m\u001b[4;94m7Efeature\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mLoOpenInAppButt\u001b[0m\n",
       "\u001b[4;94mon\u001b[0m\u001b[4;94m&%\u001b[0m\u001b[4;94m7Echannel\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mShowPostUnderCollection\u001b[0m\u001b[4;94m&%\u001b[0m\u001b[4;94m7Estage\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mmobileNavBar\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---top_nav_layout_nav------------------\u001b[0m\n",
       "\u001b[4;94m-----------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Sign \n",
       "up\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mregister\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langcha\u001b[0m\n",
       "\u001b[4;94min-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---top_nav_layout_nav---------\u001b[0m\n",
       "\u001b[4;94m--------------global_nav------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Sign \n",
       "in\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mlogin\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-\u001b[0m\n",
       "\u001b[4;94mvs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---top_nav_layout_nav------------\u001b[0m\n",
       "\u001b[4;94m-----------global_nav------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Medium Logo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---top_nav_layout_nav-----------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Write\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mregister\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fmedium.com%2Fnew-story&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=---top_\u001b[0m\n",
       "\u001b[4;94mnav_layout_nav-----------------------new_post_topnav------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Sign \n",
       "up\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mregister\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langcha\u001b[0m\n",
       "\u001b[4;94min-vs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---top_nav_layout_nav---------\u001b[0m\n",
       "\u001b[4;94m--------------global_nav------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Sign \n",
       "in\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mlogin\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-\u001b[0m\n",
       "\u001b[4;94mvs-hugging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---top_nav_layout_nav------------\u001b[0m\n",
       "\u001b[4;94m-----------global_nav------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "!|\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:64:64/1\u001b[0m*dmbNkD5D-u45r44go_cf0g.png\u001b[1m)\u001b[0m\n",
       "\n",
       "|Towards AI\n",
       "----------\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pub.towardsai.net/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---publication_nav-98111c9905da-1e9d86b1a402----------------\u001b[0m\n",
       "\u001b[4;94m-----------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "Â·\n",
       "\n",
       "|Follow \n",
       "publication\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94mactionUrl\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-\u001b[0m\n",
       "\u001b[4;94martificial-intelligence&\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mregister\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-vs-hu\u001b[0m\n",
       "\u001b[4;94mgging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&\u001b[0m\u001b[4;94mcollection\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mTowards\u001b[0m\u001b[4;94m+AI&\u001b[0m\u001b[4;94mcollectionId\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94m98111c9905da\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msource\u001b[0m\n",
       "\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---publication_nav-98111c9905da-1e9d86b1a402---------------------publication_nav------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|!|Towards \n",
       "AI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:76:76/1\u001b[0m*JyIThO-cLjlChQLb6kSlVQ.png\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pub.towardsai.net/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mp\u001b[0m\n",
       "\u001b[4;94most_page\u001b[0m\u001b[4;94m---post_publication_sidebar-98111c9905da-1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "The leading AI community and content platform focused on making AI accessible to all. Check out our new course \n",
       "platform: \u001b[1m<\u001b[0m\u001b[4;94mhttps:\u001b[0m\u001b[4;94m//academy.towardsai.net/courses/beginner-to-advanced-llm-dev\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m|Follow \u001b[0m\n",
       "\u001b[39mpublication\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/m/signin?\u001b[0m\u001b[4;94mactionUrl\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-\u001b[0m\n",
       "\u001b[4;94martificial-intelligence&\u001b[0m\u001b[4;94moperation\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mregister\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mredirect\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m%3A%2F%2Fpub.towardsai.net%2Fllamaindex-vs-langchain-vs-hu\u001b[0m\n",
       "\u001b[4;94mgging-face-smolagent-a-comprehensive-comparison-1e9d86b1a402&\u001b[0m\u001b[4;94mcollection\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mTowards\u001b[0m\u001b[4;94m+AI&\u001b[0m\u001b[4;94mcollectionId\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94m98111c9905da\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msource\u001b[0m\n",
       "\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_publication_sidebar-98111c9905da-1e9d86b1a402---------------------post_publication_sidebar-------\u001b[0m\n",
       "\u001b[4;94m-----------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex vs. LangChain vs. Hugging Face smolagent: A Comprehensive Comparison\u001b[0m\n",
       "\u001b[39m===============================================================================\u001b[0m\n",
       "\n",
       "\u001b[39m|!|Can \u001b[0m\n",
       "\u001b[39mDemir\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:64:64/1\u001b[0m\u001b[39m*w-QZ4D_8jxIZ-0iZVQiVpw.jpeg\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir13?\u001b[0m\n",
       "\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---byline--1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|Can \u001b[0m\n",
       "\u001b[39mDemir\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir13?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---byline--1e9d86b1a402--------------------------------------\u001b[0m\n",
       "\u001b[4;94m-\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m12\u001b[0m\u001b[39m min read\u001b[0m\n",
       "\n",
       "\u001b[39mÂ·\u001b[0m\n",
       "\n",
       "\u001b[39mMar \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m2025\u001b[0m\n",
       "\n",
       "\u001b[39m--\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[39mListen\u001b[0m\n",
       "\n",
       "\u001b[39mShare\u001b[0m\n",
       "\n",
       "\u001b[39m!|\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39mIntroduction\u001b[0m\n",
       "\u001b[39m============\u001b[0m\n",
       "\n",
       "\u001b[39mLarge Language Models \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mLLMs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m have opened up a new world of possibilities, powering everything from advanced \u001b[0m\n",
       "\u001b[39mchatbots to autonomous AI agents. However, to unlock their full potential, you often need robust frameworks that \u001b[0m\n",
       "\u001b[39mhandle data ingestion, prompt engineering, memory storage, and tool usage. Three significant solutions have emerged\u001b[0m\n",
       "\u001b[39min this space: **LlamaIndex**, **LangChain**, and Hugging Faceâ€™s **smolagent** approach.\u001b[0m\n",
       "\n",
       "\u001b[39mEach framework offers a unique architectural vision, performance optimization strategy, and scalability approach â€” \u001b[0m\n",
       "\u001b[39mshining in different use cases. In this article, weâ€™ll take a deep dive into all three, comparing:\u001b[0m\n",
       "\n",
       "\u001b[39m* Their design philosophies,\u001b[0m\n",
       "\u001b[39m* Real-world use cases such as information retrieval and agent development,\u001b[0m\n",
       "\u001b[39m* Pros and cons,\u001b[0m\n",
       "\u001b[39m* Guidance on choosing the one that best aligns with your project goals.\u001b[0m\n",
       "\n",
       "\u001b[39mThis tutorial-style guide aims to deliver practical insights beyond the official documentation, helping you make an\u001b[0m\n",
       "\u001b[39minformed choice for your next LLM-powered application.\u001b[0m\n",
       "\n",
       "\u001b[39mA Quick Overview of the Frameworks\u001b[0m\n",
       "\u001b[39m==================================\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mGPT Index\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m======================\u001b[0m\n",
       "\n",
       "\u001b[39m* **Core Focus**  \u001b[0m\n",
       "\u001b[39m   LlamaIndex \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mformerly GPT Index\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m specializes in efficiently connecting LLMs to external data. Its power lies in \u001b[0m\n",
       "\u001b[39mdata **indexing** and **retrieval**, allowing you to quickly query large datasets.\u001b[0m\n",
       "\u001b[39m* **How It Works**  \u001b[0m\n",
       "\u001b[39m   You feed your documents \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfiles, databases, APIs, etc.\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m into LlamaIndex to build various index structures \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mvector\u001b[0m\n",
       "\u001b[39msimilarity, keyword tables, knowledge graphs, etc.\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. When a query arrives, LlamaIndex finds and returns only the \u001b[0m\n",
       "\u001b[39mrelevant chunks to the LLM.\u001b[0m\n",
       "\u001b[39m* **Strengths**  \u001b[0m\n",
       "\u001b[39m   It excels in **retrieval-augmented generation \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mRAG\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m** scenarios, where the model requires external context to \u001b[0m\n",
       "\u001b[39mgenerate accurate answers. Itâ€™s built for enterprise-scale data \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mpotentially millions of documents\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m without \u001b[0m\n",
       "\u001b[39msacrificing performance.\u001b[0m\n",
       "\u001b[39m* **Specialization**  \u001b[0m\n",
       "\u001b[39m   LlamaIndex functions as the â€œknowledgeâ€ engine of an LLM application â€” particularly for data-heavy setups. \u001b[0m\n",
       "\u001b[39mAlthough itâ€™s expanding into agent and tool functionality, its main value remains highly efficient access to large \u001b[0m\n",
       "\u001b[39mcorpora.\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain\u001b[0m\n",
       "\u001b[39m=========\u001b[0m\n",
       "\n",
       "\u001b[39m* **Core Focus**  \u001b[0m\n",
       "\u001b[39m   LangChain provides a broad, **modular** framework for building LLM-driven applications. Its hallmark is \u001b[0m\n",
       "\u001b[39mcomposability: prompt templates, memory modules, tool usage, chain-of-thought sequences, and more.\u001b[0m\n",
       "\u001b[39m* **How It Works**  \u001b[0m\n",
       "\u001b[39m   You assemble â€œchainsâ€ of LLM interactions. For example, you might feed user input into a retrieval module, then \u001b[0m\n",
       "\u001b[39mpass the retrieved context plus user query to the LLM. Add memory for conversation context, or define agents that \u001b[0m\n",
       "\u001b[39mcan decide which external tools to call in real time.\u001b[0m\n",
       "\u001b[39m* **Strengths**  \u001b[0m\n",
       "\u001b[39m   Known as the â€œglueâ€ connecting LLMs with various data sources and APIs, LangChain shines at **multi-step \u001b[0m\n",
       "\u001b[39mreasoning** and orchestrating complex workflows. Its vast community and ecosystem mean you can plug in almost any \u001b[0m\n",
       "\u001b[39mvector database, LLM provider, or custom tool.\u001b[0m\n",
       "\u001b[39m* **Specialization**  \u001b[0m\n",
       "\u001b[39m   LangChain is a go-to solution for chatbots, question-answering systems, or any scenario that needs flexible \u001b[0m\n",
       "\u001b[39mchains of prompts, memory, and tool usage. It aims to cover â€œall things LLM,â€ from simple prototypes to \u001b[0m\n",
       "\u001b[39msophisticated production-grade workflows.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent\u001b[0m\n",
       "\u001b[39m======================\u001b[0m\n",
       "\n",
       "\u001b[39m* **Core Focus**  \u001b[0m\n",
       "\u001b[39m   Hugging Faceâ€™s **smolagent** approach \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39minitially introduced as Code Agents\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m puts a fresh spin on AI agents by \u001b[0m\n",
       "\u001b[39mhaving the LLM generate **literal Python code** to solve tasks. Instead of returning a text solution, the model can\u001b[0m\n",
       "\u001b[39mwrite and execute code that uses tools.\u001b[0m\n",
       "\u001b[39m* **How It Works**  \u001b[0m\n",
       "\u001b[39m   The agent might generate a snippet of Python \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfor instance, calling a search function, doing math, or parsing \u001b[0m\n",
       "\u001b[39mdata\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. This code is executed in a sandboxed environment, and the model uses any results for further reasoning. \u001b[0m\n",
       "\u001b[39mTools are simply Python functions/classes that the agent can call.\u001b[0m\n",
       "\u001b[39m* **Strengths**  \u001b[0m\n",
       "\u001b[39m   This approach is powerful for **multi-step tasks** requiring logic and computation. The agentâ€™s reasoning is \u001b[0m\n",
       "\u001b[39mtransparent â€” you can read the code it wrote. It also integrates smoothly with the vast Hugging Face ecosystem of \u001b[0m\n",
       "\u001b[39mmodels, datasets, and pipelines.\u001b[0m\n",
       "\u001b[39m* **Specialization**  \u001b[0m\n",
       "\u001b[39m   smolagent is excellent for open-source enthusiasts who want to avoid proprietary services and prefer the clarity\u001b[0m\n",
       "\u001b[39mof code-based reasoning. Itâ€™s still experimental, so expect rapid evolution and a smaller \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthough growing\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mcommunity.\u001b[0m\n",
       "\n",
       "\u001b[39mArchitecture and Design Philosophy\u001b[0m\n",
       "\u001b[39m==================================\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex\u001b[0m\n",
       "\u001b[39m==========\u001b[0m\n",
       "\n",
       "\u001b[39m* **Index + Query Engine**  \u001b[0m\n",
       "\u001b[39m   LlamaIndex revolves around building specialized indexes for large documents, then exposing a query engine to \u001b[0m\n",
       "\u001b[39mquickly retrieve relevant chunks. By constructing advanced data structures, LlamaIndex prevents the LLM from having\u001b[0m\n",
       "\u001b[39mto sift through massive text each time.\u001b[0m\n",
       "\u001b[39m* **Data-Centric**  \u001b[0m\n",
       "\u001b[39m   Itâ€™s all about â€œbringing your data to the LLM efficiently.â€ Indices can be vector-based for semantic search, or \u001b[0m\n",
       "\u001b[39mthey might rely on keyword matching, knowledge graphs, and so on.\u001b[0m\n",
       "\u001b[39m* **Expanding Into Agents**  \u001b[0m\n",
       "\u001b[39m   Recent releases add some agent-like features, but the primary value remains facilitating **scalable retrieval** \u001b[0m\n",
       "\u001b[39mfor LLMs in data-heavy applications.\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain\u001b[0m\n",
       "\u001b[39m=========\u001b[0m\n",
       "\n",
       "\u001b[39m* **Modular Building Blocks**  \u001b[0m\n",
       "\u001b[39m   LangChain defines interfaces for LLMs, prompts, memory modules, tools, output parsers, etc. You then build \u001b[0m\n",
       "\u001b[39mâ€œchainsâ€ that orchestrate calls across these components.\u001b[0m\n",
       "\u001b[39m* **Chains and Agents**  \u001b[0m\n",
       "\u001b[39m   A chain is a straightforward linear flow. An agent is an LLM that decides which tool \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mif any\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m to use at each \u001b[0m\n",
       "\u001b[39mstep \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39moften following the ReAct paradigm\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\u001b[39m* **Extensive Ecosystem**  \u001b[0m\n",
       "\u001b[39m   Because itâ€™s so modular, LangChain lets you easily swap out an LLM or vector database. This flexibility can be \u001b[0m\n",
       "\u001b[39mpowerful â€” but thereâ€™s also a learning curve to master all the abstractions.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent\u001b[0m\n",
       "\u001b[39m======================\u001b[0m\n",
       "\n",
       "\u001b[39m* **Code Generation Loop**  \u001b[0m\n",
       "\u001b[39m   Here, the agent is literally writing Python code that calls various tools. Each tool is a simple Python \u001b[0m\n",
       "\u001b[39mfunction, like `\u001b[0m\u001b[1;35msearch\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mquery\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` or `\u001b[0m\u001b[1;35mgenerate_image\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mprompt\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m`.\u001b[0m\n",
       "\u001b[39m* **Planning = Execution**  \u001b[0m\n",
       "\u001b[39m   The LLMâ€™s plan to solve a task directly becomes the code it writes. You can observe and debug this code, which \u001b[0m\n",
       "\u001b[39mis a unique advantage over purely prompt-based frameworks.\u001b[0m\n",
       "\u001b[39m* **Experimental Status**  \u001b[0m\n",
       "\u001b[39m   While it offers strong potential \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mespecially for advanced reasoning tasks\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, itâ€™s still early-stage. \u001b[0m\n",
       "\u001b[39mDocumentation, community support, and built-in features for memory or error handling are evolving.\u001b[0m\n",
       "\n",
       "\u001b[39m!|\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39mPerformance and Scalability\u001b[0m\n",
       "\u001b[39m===========================\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain Performance\u001b[0m\n",
       "\u001b[39m=====================\u001b[0m\n",
       "\n",
       "\u001b[39m* **Dependent on Components**  \u001b[0m\n",
       "\u001b[39m   Latency and throughput typically hinge on which LLM and data store you use, though LangChain helps with caching,\u001b[0m\n",
       "\u001b[39mbatching, and asynchronous flows.\u001b[0m\n",
       "\u001b[39m* **Horizontal Scaling**  \u001b[0m\n",
       "\u001b[39m   You can spin up multiple instances of a LangChain application and distribute requests. Each chain is relatively \u001b[0m\n",
       "\u001b[39mstateless unless you explicitly maintain memory.\u001b[0m\n",
       "\u001b[39m* **Complexity Cost**  \u001b[0m\n",
       "\u001b[39m   The more steps and calls in your chain, the higher the latency. Optimizing your chain to use only necessary \u001b[0m\n",
       "\u001b[39msteps is key.\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex Performance\u001b[0m\n",
       "\u001b[39m======================\u001b[0m\n",
       "\n",
       "\u001b[39m* **Optimized for Data Retrieval**  \u001b[0m\n",
       "\u001b[39m   By building indices upfront, LlamaIndex drastically cuts down the amount of text the LLM needs to process at \u001b[0m\n",
       "\u001b[39mquery time.\u001b[0m\n",
       "\u001b[39m* **Scales with Data**  \u001b[0m\n",
       "\u001b[39m   Indexing might be resource-intensive, but once done, queries remain fast even with millions of documents â€” \u001b[0m\n",
       "\u001b[39mperfect for large-scale knowledge bases.\u001b[0m\n",
       "\u001b[39m* **Incremental Updates**  \u001b[0m\n",
       "\u001b[39m   You can update indices periodically or in real time. For data-centric use cases, LlamaIndex often outperforms a \u001b[0m\n",
       "\u001b[39mnaive approach, especially as data volumes grow.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent Performance\u001b[0m\n",
       "\u001b[39m==================================\u001b[0m\n",
       "\n",
       "\u001b[39m* **Model + Code Execution**  \u001b[0m\n",
       "\u001b[39m   Performance depends on the selected LLMâ€™s inference speed and how complex the generated code is. Using an \u001b[0m\n",
       "\u001b[39mopen-source code model locally can be resource-heavy.\u001b[0m\n",
       "\u001b[39m* **Multi-Step Overhead**  \u001b[0m\n",
       "\u001b[39m   A React-style agent may produce multiple code snippets, each adding a new LLM call and execution time. However, \u001b[0m\n",
       "\u001b[39moffloading computations to Python might sometimes be faster than the LLM struggling with large mental math in a \u001b[0m\n",
       "\u001b[39msingle prompt.\u001b[0m\n",
       "\u001b[39m* **Scaling**  \u001b[0m\n",
       "\u001b[39m   The Hugging Face ecosystem supports containerization, on-prem, or cloud deployments. Caching and optimization \u001b[0m\n",
       "\u001b[39mstrategies for code-based agents are still developing.\u001b[0m\n",
       "\n",
       "\u001b[39m!|\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39mReal-World Implementation Examples\u001b[0m\n",
       "\u001b[39m==================================\u001b[0m\n",
       "\n",
       "\u001b[39mExample \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m: Information Retrieval \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mRAG for Q&A\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m==============================================\u001b[0m\n",
       "\n",
       "\u001b[39m**Scenario**: You have a large document corpus and want a question-answering system that can fetch relevant \u001b[0m\n",
       "\u001b[39minformation from it.\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex for Document Retrieval and Q&A\u001b[0m\n",
       "\u001b[39m-----------------------------------------\u001b[0m\n",
       "\n",
       "\u001b[39mDesigned for precisely this. A minimal code snippet:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mfrom llama_index import SimpleDirectoryReader, GPTVectorStoreIndex  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39mdocuments = \u001b[0m\u001b[1;35mSimpleDirectoryReader\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"knowledge_docs\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;35m.load_data\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mindex = \u001b[0m\u001b[1;35mGPTVectorStoreIndex.from_documents\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mdocuments\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39mquery_engine = \u001b[0m\u001b[1;35mindex.as_query_engine\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mresponse = \u001b[0m\u001b[1;35mquery_engine.query\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"What is the capital of the largest country in Europe by area?\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mresponse.response\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m* You get an answer grounded in the indexed docs, with minimal setup.\u001b[0m\n",
       "\u001b[39m* Perfect for large-scale retrieval-augmented generation.\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain for Document Retrieval and Q&A\u001b[0m\n",
       "\u001b[39m----------------------------------------\u001b[0m\n",
       "\n",
       "\u001b[39mAccomplishes the same but requires manually wiring components, for instance:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mfrom langchain.embeddings import OpenAIEmbeddings  \u001b[0m\n",
       "\u001b[39mfrom langchain.vectorstores import FAISS  \u001b[0m\n",
       "\u001b[39mfrom langchain import OpenAI  \u001b[0m\n",
       "\u001b[39mfrom langchain.chains import RetrievalQA  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39membeddings = \u001b[0m\u001b[1;35mOpenAIEmbeddings\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mvector_db = \u001b[0m\u001b[1;35mFAISS.from_texts\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mdoc_texts, \u001b[0m\u001b[33membedding\u001b[0m\u001b[39m=\u001b[0m\u001b[35membeddings\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39mllm = \u001b[0m\u001b[1;35mOpenAI\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtemperature\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mqa_chain = \u001b[0m\u001b[1;35mRetrievalQA.from_chain_type\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mllm, \u001b[0m\u001b[33mchain_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"stuff\"\u001b[0m\u001b[39m, \u001b[0m\u001b[33mretriever\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mvector_db\u001b[0m\u001b[1;35m.as_retriever\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39mresult = \u001b[0m\u001b[1;35mqa_chain.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"What is the capital of the largest country in Europe by area?\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mresult\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m* You explicitly choose embeddings and a vector store.\u001b[0m\n",
       "\u001b[39m* LangChainâ€™s flexibility is a plus, but thereâ€™s slightly more setup compared to LlamaIndexâ€™s straightforward \u001b[0m\n",
       "\u001b[39minterface.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent for Retrieval\u001b[0m\n",
       "\u001b[39m------------------------------------\u001b[0m\n",
       "\n",
       "\u001b[39msmolagent primarily focuses on code-based tool usage. A trivial example using a web search tool:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mfrom smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39magent = \u001b[0m\u001b[1;35mCodeAgent\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtools\u001b[0m\u001b[39m=|\u001b[0m\u001b[1;35mDuckDuckGoSearchTool\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mmodel\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mHfApiModel\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mquestion = \u001b[0m\u001b[32m\"What is the capital of the largest country in Europe by area?\"\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39manswer = \u001b[0m\u001b[1;35magent.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mquestion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39manswer\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m* The agent might generate Python code to search the web, parse results, and return the capital.\u001b[0m\n",
       "\u001b[39m* If you have a private corpus, youâ€™d need a custom tool \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39me.g., `LocalDocSearchTool`\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m rather than a web-based \u001b[0m\n",
       "\u001b[39msearch.\u001b[0m\n",
       "\u001b[39m* For simple Q&A, this can be overkill. It shines in multi-step tasks where code-based reasoning is advantageous.\u001b[0m\n",
       "\n",
       "\u001b[39mExample \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m: Chatbot Development \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mConversational Agents\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m======================================================\u001b[0m\n",
       "\n",
       "\u001b[39m**Scenario**: You need a conversational agent that retains context across user turns.\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain for Chatbots\u001b[0m\n",
       "\u001b[39m----------------------\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain has built-in memory modules for context management:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mfrom langchain.chat_models import ChatOpenAI  \u001b[0m\n",
       "\u001b[39mfrom langchain.chains import ConversationChain  \u001b[0m\n",
       "\u001b[39mfrom langchain.memory import ConversationBufferMemory  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39mmemory = \u001b[0m\u001b[1;35mConversationBufferMemory\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mchat_chain = \u001b[0m\u001b[1;35mConversationChain\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mllm\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mChatOpenAI\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtemperature\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, \u001b[0m\u001b[33mmemory\u001b[0m\u001b[39m=\u001b[0m\u001b[35mmemory\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;35mchat_chain.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"Hello, who are you?\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;35mchat_chain.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"Can you summarize what we've discussed so far?\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m* `ConversationChain` auto-injects previous turns into the prompt for a seamless chat experience.\u001b[0m\n",
       "\u001b[39m* You can combine memory with retrieval, tool usage, and more.\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex for Chatbots\u001b[0m\n",
       "\u001b[39m-----------------------\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex doesnâ€™t provide a full-fledged conversation flow manager out of the box. It focuses on retrieving \u001b[0m\n",
       "\u001b[39mcontext from documents. You could:\u001b[0m\n",
       "\n",
       "\u001b[39m* Use LlamaIndex to fetch relevant data each turn, then pass it to your LLM prompt.\u001b[0m\n",
       "\u001b[39m* Store or summarize conversation history manually.\u001b[0m\n",
       "\n",
       "\u001b[39mIf you only need short Q&A on a knowledge base, LlamaIndex works fine. But for a free-form chatbot with multi-turn \u001b[0m\n",
       "\u001b[39mmemory, you usually pair it with a conversation framework \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlike LangChain\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent for Chatbots\u001b[0m\n",
       "\u001b[39m-----------------------------------\u001b[0m\n",
       "\n",
       "\u001b[39msmolagent is not primarily geared toward extended dialogues with built-in memory. You can implement memory by \u001b[0m\n",
       "\u001b[39mfeeding previous turns back into the agentâ€™s prompt each time:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mprompt = \u001b[0m\u001b[32m\"User: Hello, how are you?\\nAssistant:\"\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mresponse = \u001b[0m\u001b[1;35magent.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mprompt\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mBut youâ€™ll have to track chat history yourself. The real advantage is if you want a chatbot that can **execute \u001b[0m\n",
       "\u001b[39mPython code** or call specialized tools mid-conversation. For a purely conversational use case, a simpler Chat LLM \u001b[0m\n",
       "\u001b[39mor LangChainâ€™s memory features might be more convenient.\u001b[0m\n",
       "\n",
       "\u001b[39mExample \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m: AI-Powered Agents and Tool Use\u001b[0m\n",
       "\u001b[39m=========================================\u001b[0m\n",
       "\n",
       "\u001b[39m**Scenario**: You want your LLM to not just chat but also take actions â€” calling APIs, running computations, etc.\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain Agents \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mReAct\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m------------------------\u001b[0m\n",
       "\n",
       "\u001b[39mLangChainâ€™s Agents let an LLM reason step-by-step, calling tools as needed:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mfrom langchain.llms import OpenAI  \u001b[0m\n",
       "\u001b[39mfrom langchain.agents import load_tools, initialize_agent, AgentType  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39mllm = \u001b[0m\u001b[1;35mOpenAI\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtemperature\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mtools = \u001b[0m\u001b[1;35mload_tools\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m|\u001b[0m\u001b[32m\"serpapi\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"llm-math\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mllm\u001b[0m\u001b[39m=\u001b[0m\u001b[35mllm\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39magent = \u001b[0m\u001b[1;35minitialize_agent\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mtools, llm, \u001b[0m\u001b[33magent\u001b[0m\u001b[39m=\u001b[0m\u001b[35mAgentType\u001b[0m\u001b[39m.ZERO_SHOT_REACT_DESCRIPTION, \u001b[0m\u001b[33mverbose\u001b[0m\u001b[39m=\u001b[0m\u001b[3;92mTrue\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[1;35magent.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"Who is the President of France, and what is his age multiplied by 2?\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m* The LLM decides: â€œFirst, letâ€™s use `serpapi` to find who the French president is. Next, weâ€™ll use `llm-math` to \u001b[0m\n",
       "\u001b[39mmultiply his age by \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m.â€\u001b[0m\n",
       "\u001b[39m* This is great for multi-step reasoning with any set of tools you define.\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex Within Agents\u001b[0m\n",
       "\u001b[39m------------------------\u001b[0m\n",
       "\n",
       "\u001b[39mYou can integrate LlamaIndex as a â€œtoolâ€ inside a LangChain agent. For example:\u001b[0m\n",
       "\n",
       "\u001b[39m* A â€œConsultDocsâ€ tool that internally calls `\u001b[0m\u001b[1;35mindex.query\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33m...\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m`.\u001b[0m\n",
       "\u001b[39m* When the agent needs info from your knowledge base, it uses that tool.\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex can serve as the retrieval powerhouse for an agent built in another framework.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent\u001b[0m\n",
       "\u001b[39m----------------------\u001b[0m\n",
       "\n",
       "\u001b[39mThe **code-generation** approach is especially powerful for complex tasks:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mfrom smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel  \u001b[0m\n",
       "\u001b[39m  \u001b[0m\n",
       "\u001b[39magent = \u001b[0m\u001b[1;35mCodeAgent\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mtools\u001b[0m\u001b[39m=|\u001b[0m\u001b[1;35mDuckDuckGoSearchTool\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mmodel\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mHfApiModel\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mquery = \u001b[0m\u001b[32m\"How many seconds would it take a leopard running at top speed to cross the Golden Gate Bridge?\"\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[39mresult = \u001b[0m\u001b[1;35magent.run\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mquery\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m  \u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mresult\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m* The LLM might generate Python code: searching the bridge length, top leopard speed, and then computing time = \u001b[0m\n",
       "\u001b[39mdistance \u001b[0m\u001b[35m/\u001b[0m\u001b[39m speed.\u001b[0m\n",
       "\u001b[39m* It executes that code in a sandbox. You can inspect the generated code for debugging.\u001b[0m\n",
       "\n",
       "\u001b[39mThis is particularly helpful if the agent needs to do data parsing, multiple calculations, or chain various library\u001b[0m\n",
       "\u001b[39mcalls. Itâ€™s more transparent and sometimes more accurate than standard text-based ReAct.\u001b[0m\n",
       "\n",
       "\u001b[39mPros and Cons of Each Framework\u001b[0m\n",
       "\u001b[39m===============================\u001b[0m\n",
       "\n",
       "\u001b[39mLlamaIndex\u001b[0m\n",
       "\u001b[39m==========\u001b[0m\n",
       "\n",
       "\u001b[39m**Pros**\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. **Excellent for Large-Scale Retrieval \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mRAG\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m**  \u001b[0m\n",
       "\u001b[39m    If you have a massive corpus and want fast, accurate answers, LlamaIndex is a top choice.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. **Simple API**  \u001b[0m\n",
       "\u001b[39m    A few lines of code can index documents and start answering queries.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. **Scales Gracefully**  \u001b[0m\n",
       "\u001b[39m    Pre-builds indices for quick queries even over millions of documents.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. **Interoperable**  \u001b[0m\n",
       "\u001b[39m    Works with any LLM backend and can be integrated into broader agent frameworks \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39me.g., LangChain\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m. **Rapid Feature Growth**  \u001b[0m\n",
       "\u001b[39m    Constantly adding new index types and advanced querying options.\u001b[0m\n",
       "\n",
       "\u001b[39m**Cons**\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. **Narrower Focus**  \u001b[0m\n",
       "\u001b[39m    Not a full conversation or agent framework â€” best for retrieval tasks.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. **Advanced Use Complexity**  \u001b[0m\n",
       "\u001b[39m    Tuning indexes or customizing queries can require deeper knowledge.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. **Smaller Ecosystem**  \u001b[0m\n",
       "\u001b[39m    Though growing, its community lags behind LangChainâ€™s in sheer size.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. **May Be Overkill for Small Data**  \u001b[0m\n",
       "\u001b[39m    If you only have a few pages of text, you might not need the overhead of building indices.\u001b[0m\n",
       "\n",
       "\u001b[39mLangChain\u001b[0m\n",
       "\u001b[39m=========\u001b[0m\n",
       "\n",
       "\u001b[39m**Pros**\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. **Highly Flexible and Modular**  \u001b[0m\n",
       "\u001b[39m    You can build nearly any LLM-driven workflow with its chain/agent/memory structure.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. **Rich Integrations**  \u001b[0m\n",
       "\u001b[39m    Dozens of built-in connectors for vector stores, APIs, and LLM providers.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. **Easy Prototyping**  \u001b[0m\n",
       "\u001b[39m    Many ready-to-use examples for chatbots, QA, translations, etc.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. **Built-In Memory and Prompt Handling**  \u001b[0m\n",
       "\u001b[39m    Streamlines conversation design and advanced prompting.\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m. **Large Community**  \u001b[0m\n",
       "\u001b[39m    Active forums, Discord, and third-party tutorials. Youâ€™re rarely alone in troubleshooting.\u001b[0m\n",
       "\n",
       "\u001b[39m**Cons**\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. **Can Be Overkill**  \u001b[0m\n",
       "\u001b[39m    For a single LLM call, LangChain might add unnecessary layers.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. **Steep Learning Curve**  \u001b[0m\n",
       "\u001b[39m    Fully leveraging chains, agents, memory, and tools can be complex.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. **Runtime Overhead**  \u001b[0m\n",
       "\u001b[39m    Each chain step or agent action adds latency and can complicate debugging.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. **Fast-Moving**  \u001b[0m\n",
       "\u001b[39m    Frequent updates sometimes break APIs, requiring version pinning to maintain stability.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face smolagent\u001b[0m\n",
       "\u001b[39m======================\u001b[0m\n",
       "\n",
       "\u001b[39m**Pros**\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. **Powerful, Code-Based Tool Use**  \u001b[0m\n",
       "\u001b[39m    Ideal for multi-step tasks needing calculations, data manipulation, or advanced APIs.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. **Transparency and Debuggability**  \u001b[0m\n",
       "\u001b[39m    You can read the Python code the model writes, making error analysis easier.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. **Leverages Hugging Face Ecosystem**  \u001b[0m\n",
       "\u001b[39m    Integrate any HF model or pipeline, plus open-source flexibility.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. **No Vendor Lock-In**  \u001b[0m\n",
       "\u001b[39m    Fully open-source. You can self-host everything if you prefer.\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m. **Code-Centric Approach**  \u001b[0m\n",
       "\u001b[39m    Offloading complexity to Python can boost accuracy for tasks like math or structured data handling.\u001b[0m\n",
       "\n",
       "\u001b[39m**Cons**\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. **Experimental**  \u001b[0m\n",
       "\u001b[39m    The API is still evolving, and the community is smaller than LangChainâ€™s.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. **Performance Overheads**  \u001b[0m\n",
       "\u001b[39m    Generating and executing code can introduce additional latency, especially in multi-step loops.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. **Setup Complexity**  \u001b[0m\n",
       "\u001b[39m    Sandboxing code, defining tools well, and preventing malicious/unsafe code requires extra care.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. **Not Primarily Conversation-Focused**  \u001b[0m\n",
       "\u001b[39m    Out-of-the-box memory for long dialogues doesnâ€™t exist; you must build it yourself.\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m. **Uncertain Failure Modes**  \u001b[0m\n",
       "\u001b[39m    If the model generates incorrect or buggy code, you need error-handling strategies \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlike retries or \u001b[0m\n",
       "\u001b[39mself-correction\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39mChoosing the Right Framework\u001b[0m\n",
       "\u001b[39m============================\u001b[0m\n",
       "\n",
       "\u001b[39mHow do you pick between these three? Here are some guidelines:\u001b[0m\n",
       "\n",
       "\u001b[39m**â€œI have a huge corpus and need a Q&A system over it.â€**\u001b[0m\n",
       "\n",
       "\u001b[39m* ***LlamaIndex*** *is your go-to. Itâ€™s purpose-built for speedy retrieval in large-scale data scenarios.*\u001b[0m\n",
       "\n",
       "\u001b[39m**â€œI want a general chatbot/tool application with multiple LLM interactions and memory.â€**\u001b[0m\n",
       "\n",
       "\u001b[39m* ***LangChain*** *provides modular building blocks for all sorts of LLM chains and agents, plus community \u001b[0m\n",
       "\u001b[39msupport.*\u001b[0m\n",
       "\n",
       "\u001b[39m**â€œMy AI needs to perform complex, multi-step tasks with open-source flexibility.â€**\u001b[0m\n",
       "\n",
       "\u001b[39m* ***Hugging Face smolagent****. If code-based logic is appealing \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39me.g., advanced calculations, dynamic Python \u001b[0m\n",
       "\u001b[39musage\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, youâ€™ll love smolagent.*\u001b[0m\n",
       "\n",
       "\u001b[39m**â€œI need something quick with minimal coding.â€**\u001b[0m\n",
       "\n",
       "\u001b[39m* ***LangChain*** *or* ***LlamaIndex****. For a simple Q&A prototype, both are straightforward. Pick LlamaIndex if \u001b[0m\n",
       "\u001b[39mdata is huge; go with LangChain if you also need robust conversation or tool usage.*\u001b[0m\n",
       "\n",
       "\u001b[39m**â€œProduction-grade stability with strong support.â€**\u001b[0m\n",
       "\n",
       "\u001b[39m* ***LangChain*** *currently boasts the largest ecosystem and community;* ***LlamaIndex*** *is stable for retrieval\u001b[0m\n",
       "\u001b[39mtasks.* ***smolagent*** *is still maturing, so it might require more engineering effort for production.*\u001b[0m\n",
       "\n",
       "\u001b[39mUltimately, these frameworks can be complementary. Many teams, for instance, combine LlamaIndex \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfor data \u001b[0m\n",
       "\u001b[39mretrieval\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and LangChain \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfor conversation/agents\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. You could even incorporate smolagent for code-based logic in \u001b[0m\n",
       "\u001b[39mspecialized tasks. Usually, though, youâ€™ll choose one as the primary backbone and then pull in features from the \u001b[0m\n",
       "\u001b[39mothers if needed.\u001b[0m\n",
       "\n",
       "\u001b[39mConclusion\u001b[0m\n",
       "\u001b[39m==========\u001b[0m\n",
       "\n",
       "\u001b[39mThe LLM application development landscape is advancing rapidly, with **LlamaIndex**, **LangChain**, and **Hugging \u001b[0m\n",
       "\u001b[39mFace smolagent** offering three compelling approaches. Weâ€™ve covered:\u001b[0m\n",
       "\n",
       "\u001b[39m* How **LlamaIndex** excels at fast and scalable data retrieval,\u001b[0m\n",
       "\u001b[39m* How **LangChain** provides a flexible, chain-of-thought framework for everything from simple chat to multi-step \u001b[0m\n",
       "\u001b[39magent orchestration,\u001b[0m\n",
       "\u001b[39m* How **smolagent**â€™s code-generation paradigm can handle advanced logic and tool usage with remarkable \u001b[0m\n",
       "\u001b[39mtransparency.\u001b[0m\n",
       "\n",
       "\u001b[39mEach framework has its own strengths and trade-offs â€” what matters is matching them to your projectâ€™s exact \u001b[0m\n",
       "\u001b[39mrequirements. The good news? All three are open source and easy to try. If youâ€™re unsure, spin up a quick prototype\u001b[0m\n",
       "\u001b[39min each, see which one feels most natural and performs best for your needs.\u001b[0m\n",
       "\n",
       "\u001b[39mIn this era of rapid innovation, frameworks may come and go, but the core principle remains: identify your \u001b[0m\n",
       "\u001b[39mproblemâ€™s priorities \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mdata scale, complexity, conversation vs. tool usage\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and pick the framework that aligns with \u001b[0m\n",
       "\u001b[39mthose needs. LlamaIndex, LangChain, and smolagent are each brilliant in their own domain. Armed with the insights \u001b[0m\n",
       "\u001b[39mfrom this comparison, youâ€™ll be well-equipped to make your next LLM project a success.\u001b[0m\n",
       "\n",
       "\u001b[39mHappy building!\u001b[0m\n",
       "\n",
       "\u001b[39m|Llamaindex\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/tag/llamaindex?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402-----------------------------------\u001b[0m\n",
       "\u001b[4;94m----\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|Langchain \u001b[0m\n",
       "\u001b[39mAgents\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/tag/langchain-agents?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402----------------------------------\u001b[0m\n",
       "\u001b[4;94m-----\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|Smolagents\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/tag/smolagents?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402-----------------------------------\u001b[0m\n",
       "\u001b[4;94m----\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|Agentic \u001b[0m\n",
       "\u001b[39mAi\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/tag/agentic-ai?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|Llm \u001b[0m\n",
       "\u001b[39mApplications\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://medium.com/tag/llm-applications?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402----------------------------\u001b[0m\n",
       "\u001b[4;94m-----------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m--\u001b[0m\n",
       "\n",
       "\u001b[39m--\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[39m|!|Towards \u001b[0m\n",
       "\u001b[39mAI\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:96:96/1\u001b[0m\u001b[39m*JyIThO-cLjlChQLb6kSlVQ.png\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://pub.towardsai.net/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mp\u001b[0m\n",
       "\u001b[4;94most_page\u001b[0m\u001b[4;94m---post_publication_info--1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|!|Towards \u001b[0m\n",
       "\u001b[39mAI\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:128:128/1\u001b[0m\u001b[39m*JyIThO-cLjlChQLb6kSlVQ.png\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://pub.towardsai.net/?\u001b[0m\u001b[4;94msource\u001b[0m\n",
       "\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_publication_info--1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39mFollow\u001b[0m\n",
       "\n",
       "\u001b[39m|Published in Towards AI\u001b[0m\n",
       "\u001b[39m-----------------------\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://pub.towardsai.net/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_publication_info--1e9d86b1a402---------\u001b[0m\n",
       "\u001b[4;94m------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m|83K \u001b[0m\n",
       "\u001b[39mfollowers\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfollowers\u001b[0m\u001b[39m?\u001b[0m\u001b[33msource\u001b[0m\u001b[39m=\u001b[0m\u001b[35mpost_page\u001b[0m\u001b[39m---post_publication_info--1e9d86b1a402---------------------------------------\u001b[0m\n",
       "\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39mÂ·|Last publishedÂ \u001b[0m\u001b[1;36m15\u001b[0m\u001b[39m hours \u001b[0m\n",
       "\u001b[39mago\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhow-to-really-build-on-top-of-llms-07822726c228\u001b[0m\u001b[39m?\u001b[0m\u001b[33msource\u001b[0m\u001b[39m=\u001b[0m\u001b[35mpost_page\u001b[0m\u001b[39m---post_publication_info--1e9d86b1a402-------\u001b[0m\n",
       "\u001b[39m--------------------------------\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39mThe leading AI community and content platform focused on making AI accessible to all. Check out our new course \u001b[0m\n",
       "\u001b[39mplatform: <\u001b[0m\u001b[4;94mhttps://academy.towardsai.net/courses/beginner-to-advanced-llm-dev\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Follow\n",
       "\n",
       "|!|Can \n",
       "Demir\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:96:96/1\u001b[0m*w-QZ4D_8jxIZ-0iZVQiVpw.jpeg\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir13?\u001b[0m\n",
       "\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_author_info--1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|!|Can \n",
       "Demir\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://miro.medium.com/v2/resize:fill:128:128/1\u001b[0m*w-QZ4D_8jxIZ-0iZVQiVpw.jpeg\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir1\u001b[0m\n",
       "\u001b[4;94m3?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_author_info--1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Written by Can Demir\n",
       "--------------------\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir13?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_author_info--1e9d86b1a402-------------\u001b[0m\n",
       "\u001b[4;94m--------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|\u001b[1;36m22\u001b[0m \n",
       "followers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir13/followers?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_author_info--1e9d86b1a402--------------\u001b[0m\n",
       "\u001b[4;94m-------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "Â·|\u001b[1;36m39\u001b[0m \n",
       "following\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/@candemir13/following?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m---post_author_info--1e9d86b1a402--------------\u001b[0m\n",
       "\u001b[4;94m-------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "AI/ML Engineer & Data Professional with finance background. Developing enterprise-scale data & AI solutions. \n",
       "Exploring tech innovations ğŸ“ˆ\n",
       "\n",
       "Responses \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------\n",
       "\n",
       "See all responses\n",
       "\n",
       "|Help\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://help.medium.com/hc/en-us?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Status\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.statuspage.io/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|About\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/about?\u001b[0m\u001b[4;94mautoplay\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94m1\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402--------------------------------------\u001b[0m\n",
       "\u001b[4;94m-\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Careers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402----------\u001b[0m\n",
       "\u001b[4;94m-----------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Press\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmailto:pressinquiries@medium.com\u001b[1m)\u001b[0m\n",
       "\n",
       "|Blog\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://blog.medium.com/?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Privacy\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://policy.medium.com/medium-privacy-policy-f03bf92035c9?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402-----------\u001b[0m\n",
       "\u001b[4;94m----------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Rules\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://policy.medium.com/medium-rules-30e5502c4eb4?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402----------------------\u001b[0m\n",
       "\u001b[4;94m-----------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Terms\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://policy.medium.com/medium-terms-of-service-9db0094a1e0f?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402-----------\u001b[0m\n",
       "\u001b[4;94m----------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "|Text to \n",
       "speech\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://speechify.com/medium?\u001b[0m\u001b[4;94msource\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mpost_page\u001b[0m\u001b[4;94m-----1e9d86b1a402---------------------------------------\u001b[0m\u001b[4;94m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.42 seconds| Input tokens: 4,402 | Output tokens: 111]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 4.42 seconds| Input tokens: 4,402 | Output tokens: 111]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Output message of the LLM:</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">ModelResponse(id='chatcmpl-BqFaAfFW6B9IDEEwwxhSgZoIxGB5P', created=1751792654, model='gpt-4.1-mini-2025-04-14', </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">object='chat.completion', system_fingerprint='fp_6f2eabb9a5', choices=[Choices(finish_reason='tool_calls', index=0,</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">message=Message(content=None, role='assistant', </span><span style=\"background-color: #0d1117\">                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"answer\":\"### 1. Task outcome (short </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">version):\\\\nThe main differences between smol-agent and LangChain lie in their design philosophy, usage approach, </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">and target use cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">memory, and tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">focuses on LLMs generating and executing Python code for task-solving, offering transparency and power for </span><span style=\"background-color: #0d1117\">        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">code-based reasoning and multi-step computations, suited for open-source enthusiasts and advanced logic </span><span style=\"background-color: #0d1117\">           </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">tasks.\\\\n\\\\n### 2. Task outcome (extremely detailed version):\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Design Philosophies:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- LangChain is </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">modular, providing building blocks for LLM interactions, memory modules, tool usage, and chaining operations. It is</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">the â€œglueâ€ for orchestrating complex workflows involving LLMs, vector stores, and APIs.\\\\n- Smol-agent uses a </span><span style=\"background-color: #0d1117\">     </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">code-generation paradigm where the LLM generates Python code snippets dynamically to solve tasks by directly </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">calling Python functions as tools, enabling transparent and debuggable multi-step reasoning.\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Strengths &amp; </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">Usage:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented </span><span style=\"background-color: #0d1117\">        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">generation workflows, and scenarios requiring orchestration of multiple tools or APIs.\\\\n- Smol-agent is ideal for </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls via executable Python </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">code in sandbox environments.\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Performance &amp; Scalability:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- LangChain depends on the LLM and data store </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">choice; it supports asynchronous workflows and horizontal scaling but may incur runtime overhead due to multiple </span><span style=\"background-color: #0d1117\">  </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">chain steps.\\\\n- Smol-agent performance depends on model speed and code complexity; generating and running code </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">introduces some latency but can offload complex tasks efficiently.\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Community &amp; Maturity:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- LangChain has</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">a large, active community, widespread adoption, and many integrations.\\\\n- Smol-agent is newer, experimental, with </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">a smaller but growing community, focusing on open-source flexibility.\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Use Cases Comparison:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- Use </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and workflow </span><span style=\"background-color: #0d1117\">               </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">orchestration.\\\\n- Use smol-agent when tasks demand precise code execution, advanced math, data parsing, or when </span><span style=\"background-color: #0d1117\">  </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">preferring open-source code-centric workflows.\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Example Contexts:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- For large document retrieval Q&amp;A, </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">LangChain or LlamaIndex are straightforward,\\\\n- For AI agents that execute API calls or compute complex </span><span style=\"background-color: #0d1117\">          </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">operations, smol-agent shines by code generation.\\\\n\\\\n</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Limitations:**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">\\\\n- LangChain may have a steep learning </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">curve and runtime latency.\\\\n- Smol-agent requires careful sandboxing and error handling for generated code; memory</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">features for chatbots are limited.\\\\n\\\\n### 3. Additional context (if relevant):\\\\nMany projects combine these </span><span style=\"background-color: #0d1117\">    </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for conversational orchestration, </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">or integrating smol-agent for specialized code-based reasoning tasks within broader pipelines. The choice depends </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">on your projectâ€™s priorities: data scale, complexity of reasoning, preference for code vs prompt-based logic, and </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">production maturity needs.\"}', name='final_answer'), id='call_iwWXjs4qJitr45Bc3WrBqZIB', type='function')], </span><span style=\"background-color: #0d1117\">       </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">usage=Usage(completion_tokens=666, prompt_tokens=10078, total_tokens=10744, </span><span style=\"background-color: #0d1117\">                                       </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, </span><span style=\"background-color: #0d1117\">            </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), </span><span style=\"background-color: #0d1117\">                                              </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=3072, text_tokens=None, </span><span style=\"background-color: #0d1117\">            </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">image_tokens=None)), service_tier='default')</span><span style=\"background-color: #0d1117\">                                                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3mOutput message of the LLM:\u001b[0m \u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mModelResponse(id='chatcmpl-BqFaAfFW6B9IDEEwwxhSgZoIxGB5P',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcreated=1751792654,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel='gpt-4.1-mini-2025-04-14',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mobject='chat.completion',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msystem_fingerprint='fp_6f2eabb9a5',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchoices=[Choices(finish_reason='tool_calls',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mindex=0,\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mmessage=Message(content=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrole='assistant',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"answer\":\"###\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m1.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTask\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moutcome\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(short\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mversion):\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbetween\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlie\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtheir\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdesign\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mphilosophy,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23musage\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapproach,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtarget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcases.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodular\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mframework\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbuilding\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLM-driven\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapplications\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwith\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mflexible\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchains,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mmemory,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23musage,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexcelling\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchatbot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdevelopment.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(by\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mHugging\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFace)\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mfocuses\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLMs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgenerating\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexecuting\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mPython\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtask-solving,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moffering\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransparency\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpower\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcode-based\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputations,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuited\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mopen-source\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23menthusiasts\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madvanced\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlogic\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m           \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtasks.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn###\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m2.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTask\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moutcome\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(extremely\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetailed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mversion):\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Design Philosophies:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mmodular,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mproviding\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbuilding\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mblocks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLM\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minteractions,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodules,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23musage,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchaining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moperations.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mIt\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mâ€œglueâ€\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23morchestrating\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflows\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minvolving\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLMs,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mvector\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstores,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAPIs.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muses\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m     \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcode-generation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mparadigm\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhere\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLM\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgenerates\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mPython\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msnippets\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdynamically\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msolve\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtasks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mby\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdirectly\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcalling\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mPython\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfunctions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mas\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtools,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23menabling\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransparent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdebuggable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Strengths & \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[1;38;2;230;237;243;48;2;13;17;23mUsage:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexcels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbuilding\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconversational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magents,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchatbots\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwith\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieval-augmented\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mgeneration\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflows,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscenarios\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrequiring\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23morchestration\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmultiple\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtools\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAPIs.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mideal\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtasks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrequiring\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcalculations,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmanipulation,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madvanced\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlogic,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcalls\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mvia\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexecutable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mPython\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msandbox\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23menvironments.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Performance & Scalability:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdepends\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLM\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mchoice;\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msupports\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23masynchronous\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflows\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhorizontal\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscaling\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbut\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmay\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mincur\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mruntime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moverhead\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdue\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmultiple\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mchain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msteps.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mperformance\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdepends\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspeed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplexity;\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgenerating\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrunning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mintroduces\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msome\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlatency\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbut\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcan\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moffload\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtasks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mefficiently.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Community & Maturity:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhas\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlarge,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mactive\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcommunity,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwidespread\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madoption,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmany\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mintegrations.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnewer,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexperimental,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwith\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmaller\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbut\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgrowing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcommunity,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfocusing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mopen-source\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mflexibility.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Use Cases Comparison:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mUse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchatbots\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrequiring\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconversational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23musage,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m               \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23morchestration.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mUse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhen\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtasks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdemand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprecise\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexecution,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madvanced\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmath,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mparsing,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhen\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mpreferring\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mopen-source\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode-centric\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflows.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Example Contexts:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlarge\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocument\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieval\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mQ&A,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLlamaIndex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mare\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstraightforward,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mFor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magents\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexecute\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAPI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcalls\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompute\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m          \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23moperations,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mshines\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mby\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgeneration.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Limitations:**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmay\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhave\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msteep\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlearning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcurve\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mruntime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlatency.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrequires\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcareful\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msandboxing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23merror\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhandling\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgenerated\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode;\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mfeatures\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchatbots\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mare\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlimited.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mn###\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m3.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAdditional\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcontext\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(if\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant):\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\\\\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnMany\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprojects\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcombine\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthese\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m    \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mframeworks;\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23me.g.,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLlamaIndex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscalable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocument\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieval\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23malongside\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconversational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23morchestration,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mintegrating\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecialized\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode-based\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtasks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwithin\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbroader\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpipelines.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchoice\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdepends\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23myour\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprojectâ€™s\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpriorities:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscale,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplexity\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpreference\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mvs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprompt-based\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlogic,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mproduction\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmaturity\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneeds.\"}',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mname='final_answer'),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mid='call_iwWXjs4qJitr45Bc3WrBqZIB',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtype='function')],\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m       \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mfunction_call=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovider_specific_fields={'refusal':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mNone},\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mannotations=[]),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovider_specific_fields={})],\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23musage=Usage(completion_tokens=666,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprompt_tokens=10078,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtotal_tokens=10744,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                       \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mcompletion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23maudio_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m            \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mreasoning_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrejected_prediction_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtext_tokens=None),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m                                              \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcached_tokens=3072,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtext_tokens=None,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m            \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mimage_tokens=None)),\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mservice_tier='default')\u001b[0m\u001b[48;2;13;17;23m                                                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'final_answer' with arguments: {'answer': '### 1. Task outcome (short version):\\nThe main         â”‚\n",
       "â”‚ differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target use     â”‚\n",
       "â”‚ cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and  â”‚\n",
       "â”‚ tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on  â”‚\n",
       "â”‚ LLMs generating and executing Python code for task-solving, offering transparency and power for code-based      â”‚\n",
       "â”‚ reasoning and multi-step computations, suited for open-source enthusiasts and advanced logic tasks.\\n\\n### 2.   â”‚\n",
       "â”‚ Task outcome (extremely detailed version):\\n\\n**Design Philosophies:**\\n- LangChain is modular, providing       â”‚\n",
       "â”‚ building blocks for LLM interactions, memory modules, tool usage, and chaining operations. It is the â€œglueâ€ for â”‚\n",
       "â”‚ orchestrating complex workflows involving LLMs, vector stores, and APIs.\\n- Smol-agent uses a code-generation   â”‚\n",
       "â”‚ paradigm where the LLM generates Python code snippets dynamically to solve tasks by directly calling Python     â”‚\n",
       "â”‚ functions as tools, enabling transparent and debuggable multi-step reasoning.\\n\\n**Strengths &amp; Usage:**\\n-      â”‚\n",
       "â”‚ LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation        â”‚\n",
       "â”‚ workflows, and scenarios requiring orchestration of multiple tools or APIs.\\n- Smol-agent is ideal for          â”‚\n",
       "â”‚ multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls via executable Python â”‚\n",
       "â”‚ code in sandbox environments.\\n\\n**Performance &amp; Scalability:**\\n- LangChain depends on the LLM and data store  â”‚\n",
       "â”‚ choice; it supports asynchronous workflows and horizontal scaling but may incur runtime overhead due to         â”‚\n",
       "â”‚ multiple chain steps.\\n- Smol-agent performance depends on model speed and code complexity; generating and      â”‚\n",
       "â”‚ running code introduces some latency but can offload complex tasks efficiently.\\n\\n**Community &amp; Maturity:**\\n- â”‚\n",
       "â”‚ LangChain has a large, active community, widespread adoption, and many integrations.\\n- Smol-agent is newer,    â”‚\n",
       "â”‚ experimental, with a smaller but growing community, focusing on open-source flexibility.\\n\\n**Use Cases         â”‚\n",
       "â”‚ Comparison:**\\n- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and â”‚\n",
       "â”‚ workflow orchestration.\\n- Use smol-agent when tasks demand precise code execution, advanced math, data         â”‚\n",
       "â”‚ parsing, or when preferring open-source code-centric workflows.\\n\\n**Example Contexts:**\\n- For large document  â”‚\n",
       "â”‚ retrieval Q&amp;A, LangChain or LlamaIndex are straightforward,\\n- For AI agents that execute API calls or compute  â”‚\n",
       "â”‚ complex operations, smol-agent shines by code generation.\\n\\n**Limitations:**\\n- LangChain may have a steep     â”‚\n",
       "â”‚ learning curve and runtime latency.\\n- Smol-agent requires careful sandboxing and error handling for generated  â”‚\n",
       "â”‚ code; memory features for chatbots are limited.\\n\\n### 3. Additional context (if relevant):\\nMany projects      â”‚\n",
       "â”‚ combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for              â”‚\n",
       "â”‚ conversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within       â”‚\n",
       "â”‚ broader pipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning,        â”‚\n",
       "â”‚ preference for code vs prompt-based logic, and production maturity needs.'}                                     â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Calling tool: 'final_answer' with arguments: {'answer': '### 1. Task outcome (short version):\\nThe main         â”‚\n",
       "â”‚ differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target use     â”‚\n",
       "â”‚ cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and  â”‚\n",
       "â”‚ tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on  â”‚\n",
       "â”‚ LLMs generating and executing Python code for task-solving, offering transparency and power for code-based      â”‚\n",
       "â”‚ reasoning and multi-step computations, suited for open-source enthusiasts and advanced logic tasks.\\n\\n### 2.   â”‚\n",
       "â”‚ Task outcome (extremely detailed version):\\n\\n**Design Philosophies:**\\n- LangChain is modular, providing       â”‚\n",
       "â”‚ building blocks for LLM interactions, memory modules, tool usage, and chaining operations. It is the â€œglueâ€ for â”‚\n",
       "â”‚ orchestrating complex workflows involving LLMs, vector stores, and APIs.\\n- Smol-agent uses a code-generation   â”‚\n",
       "â”‚ paradigm where the LLM generates Python code snippets dynamically to solve tasks by directly calling Python     â”‚\n",
       "â”‚ functions as tools, enabling transparent and debuggable multi-step reasoning.\\n\\n**Strengths & Usage:**\\n-      â”‚\n",
       "â”‚ LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation        â”‚\n",
       "â”‚ workflows, and scenarios requiring orchestration of multiple tools or APIs.\\n- Smol-agent is ideal for          â”‚\n",
       "â”‚ multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls via executable Python â”‚\n",
       "â”‚ code in sandbox environments.\\n\\n**Performance & Scalability:**\\n- LangChain depends on the LLM and data store  â”‚\n",
       "â”‚ choice; it supports asynchronous workflows and horizontal scaling but may incur runtime overhead due to         â”‚\n",
       "â”‚ multiple chain steps.\\n- Smol-agent performance depends on model speed and code complexity; generating and      â”‚\n",
       "â”‚ running code introduces some latency but can offload complex tasks efficiently.\\n\\n**Community & Maturity:**\\n- â”‚\n",
       "â”‚ LangChain has a large, active community, widespread adoption, and many integrations.\\n- Smol-agent is newer,    â”‚\n",
       "â”‚ experimental, with a smaller but growing community, focusing on open-source flexibility.\\n\\n**Use Cases         â”‚\n",
       "â”‚ Comparison:**\\n- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and â”‚\n",
       "â”‚ workflow orchestration.\\n- Use smol-agent when tasks demand precise code execution, advanced math, data         â”‚\n",
       "â”‚ parsing, or when preferring open-source code-centric workflows.\\n\\n**Example Contexts:**\\n- For large document  â”‚\n",
       "â”‚ retrieval Q&A, LangChain or LlamaIndex are straightforward,\\n- For AI agents that execute API calls or compute  â”‚\n",
       "â”‚ complex operations, smol-agent shines by code generation.\\n\\n**Limitations:**\\n- LangChain may have a steep     â”‚\n",
       "â”‚ learning curve and runtime latency.\\n- Smol-agent requires careful sandboxing and error handling for generated  â”‚\n",
       "â”‚ code; memory features for chatbots are limited.\\n\\n### 3. Additional context (if relevant):\\nMany projects      â”‚\n",
       "â”‚ combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for              â”‚\n",
       "â”‚ conversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within       â”‚\n",
       "â”‚ broader pipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning,        â”‚\n",
       "â”‚ preference for code vs prompt-based logic, and production maturity needs.'}                                     â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: ### 1. Task outcome (short version):</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">The main differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">use cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on LLMs</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">generating and executing Python code for task-solving, offering transparency and power for code-based reasoning and</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">multi-step computations, suited for open-source enthusiasts and advanced logic tasks.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 2. Task outcome (extremely detailed version):</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Design Philosophies:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- LangChain is modular, providing building blocks for LLM interactions, memory modules, tool usage, and chaining </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">operations. It is the â€œglueâ€ for orchestrating complex workflows involving LLMs, vector stores, and APIs.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Smol-agent uses a code-generation paradigm where the LLM generates Python code snippets dynamically to solve </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">tasks by directly calling Python functions as tools, enabling transparent and debuggable multi-step reasoning.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Strengths &amp; Usage:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">workflows, and scenarios requiring orchestration of multiple tools or APIs.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Smol-agent is ideal for multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">via executable Python code in sandbox environments.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Performance &amp; Scalability:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- LangChain depends on the LLM and data store choice; it supports asynchronous workflows and horizontal scaling but</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">may incur runtime overhead due to multiple chain steps.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Smol-agent performance depends on model speed and code complexity; generating and running code introduces some </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">latency but can offload complex tasks efficiently.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Community &amp; Maturity:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- LangChain has a large, active community, widespread adoption, and many integrations.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Smol-agent is newer, experimental, with a smaller but growing community, focusing on open-source flexibility.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Use Cases Comparison:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and workflow </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">orchestration.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Use smol-agent when tasks demand precise code execution, advanced math, data parsing, or when preferring </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">open-source code-centric workflows.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Example Contexts:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- For large document retrieval Q&amp;A, LangChain or LlamaIndex are straightforward,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- For AI agents that execute API calls or compute complex operations, smol-agent shines by code generation.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">**Limitations:**</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- LangChain may have a steep learning curve and runtime latency.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Smol-agent requires careful sandboxing and error handling for generated code; memory features for chatbots are </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">limited.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">### 3. Additional context (if relevant):</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Many projects combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">conversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within broader </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">pipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning, preference for </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">code vs prompt-based logic, and production maturity needs.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: ### 1. Task outcome (short version):\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mThe main differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target \u001b[0m\n",
       "\u001b[1;38;2;212;183;2muse cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on LLMs\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mgenerating and executing Python code for task-solving, offering transparency and power for code-based reasoning and\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmulti-step computations, suited for open-source enthusiasts and advanced logic tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 2. Task outcome (extremely detailed version):\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Design Philosophies:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- LangChain is modular, providing building blocks for LLM interactions, memory modules, tool usage, and chaining \u001b[0m\n",
       "\u001b[1;38;2;212;183;2moperations. It is the â€œglueâ€ for orchestrating complex workflows involving LLMs, vector stores, and APIs.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Smol-agent uses a code-generation paradigm where the LLM generates Python code snippets dynamically to solve \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtasks by directly calling Python functions as tools, enabling transparent and debuggable multi-step reasoning.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Strengths & Usage:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mworkflows, and scenarios requiring orchestration of multiple tools or APIs.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Smol-agent is ideal for multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mvia executable Python code in sandbox environments.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Performance & Scalability:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- LangChain depends on the LLM and data store choice; it supports asynchronous workflows and horizontal scaling but\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmay incur runtime overhead due to multiple chain steps.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Smol-agent performance depends on model speed and code complexity; generating and running code introduces some \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlatency but can offload complex tasks efficiently.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Community & Maturity:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- LangChain has a large, active community, widespread adoption, and many integrations.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Smol-agent is newer, experimental, with a smaller but growing community, focusing on open-source flexibility.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Use Cases Comparison:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and workflow \u001b[0m\n",
       "\u001b[1;38;2;212;183;2morchestration.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Use smol-agent when tasks demand precise code execution, advanced math, data parsing, or when preferring \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mopen-source code-centric workflows.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Example Contexts:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- For large document retrieval Q&A, LangChain or LlamaIndex are straightforward,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- For AI agents that execute API calls or compute complex operations, smol-agent shines by code generation.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m**Limitations:**\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- LangChain may have a steep learning curve and runtime latency.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Smol-agent requires careful sandboxing and error handling for generated code; memory features for chatbots are \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlimited.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m### 3. Additional context (if relevant):\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mMany projects combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mconversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within broader \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning, preference for \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcode vs prompt-based logic, and production maturity needs.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 13.78 seconds| Input tokens: 14,480 | Output tokens: 777]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 13.78 seconds| Input tokens: 14,480 | Output tokens: 777]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Here is the final answer from your managed agent 'search_agent':\n",
       "### 1. Task outcome (short version):\n",
       "The main differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target \n",
       "use cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and \n",
       "tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on LLMs\n",
       "generating and executing Python code for task-solving, offering transparency and power for code-based reasoning and\n",
       "multi-step computations, suited for open-source enthusiasts and advanced logic tasks.\n",
       "\n",
       "### 2. Task outcome (extremely detailed version):\n",
       "\n",
       "**Design Philosophies:**\n",
       "- LangChain is modular, providing building blocks for LLM interactions, memory modules, tool usage, and chaining \n",
       "operations. It is the â€œglueâ€ for orchestrating complex workflows involving LLMs, vector stores, and APIs.\n",
       "- Smol-agent uses a code-generation paradigm where the LLM generates Python code snippets dynamically to solve \n",
       "tasks by directly calling Python functions as tools, enabling transparent and debuggable multi-step reasoning.\n",
       "\n",
       "**Strengths &amp; Usage:**\n",
       "- LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation \n",
       "workflows, and scenarios requiring orchestration of multiple tools or APIs.\n",
       "- Smol-agent is ideal for multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls\n",
       "via executable Python code in sandbox environments.\n",
       "\n",
       "**Performance &amp; Scalability:**\n",
       "- LangChain depends on the LLM and data store choice; it supports asynchronous workflows and horizontal scaling but\n",
       "may incur runtime overhead due to multiple chain steps.\n",
       "- Smol-agent performance depends on model speed and code complexity; generating and running code introduces some \n",
       "latency but can offload complex tasks efficiently.\n",
       "\n",
       "**Community &amp; Maturity:**\n",
       "- LangChain has a large, active community, widespread adoption, and many integrations.\n",
       "- Smol-agent is newer, experimental, with a smaller but growing community, focusing on open-source flexibility.\n",
       "\n",
       "**Use Cases Comparison:**\n",
       "- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and workflow \n",
       "orchestration.\n",
       "- Use smol-agent when tasks demand precise code execution, advanced math, data parsing, or when preferring \n",
       "open-source code-centric workflows.\n",
       "\n",
       "**Example Contexts:**\n",
       "- For large document retrieval Q&amp;A, LangChain or LlamaIndex are straightforward,\n",
       "- For AI agents that execute API calls or compute complex operations, smol-agent shines by code generation.\n",
       "\n",
       "**Limitations:**\n",
       "- LangChain may have a steep learning curve and runtime latency.\n",
       "- Smol-agent requires careful sandboxing and error handling for generated code; memory features for chatbots are \n",
       "limited.\n",
       "\n",
       "### 3. Additional context (if relevant):\n",
       "Many projects combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for \n",
       "conversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within broader \n",
       "pipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning, preference for \n",
       "code vs prompt-based logic, and production maturity needs.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Here is the final answer from your managed agent 'search_agent':\n",
       "### 1. Task outcome (short version):\n",
       "The main differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target \n",
       "use cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and \n",
       "tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on LLMs\n",
       "generating and executing Python code for task-solving, offering transparency and power for code-based reasoning and\n",
       "multi-step computations, suited for open-source enthusiasts and advanced logic tasks.\n",
       "\n",
       "### 2. Task outcome (extremely detailed version):\n",
       "\n",
       "**Design Philosophies:**\n",
       "- LangChain is modular, providing building blocks for LLM interactions, memory modules, tool usage, and chaining \n",
       "operations. It is the â€œglueâ€ for orchestrating complex workflows involving LLMs, vector stores, and APIs.\n",
       "- Smol-agent uses a code-generation paradigm where the LLM generates Python code snippets dynamically to solve \n",
       "tasks by directly calling Python functions as tools, enabling transparent and debuggable multi-step reasoning.\n",
       "\n",
       "**Strengths & Usage:**\n",
       "- LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation \n",
       "workflows, and scenarios requiring orchestration of multiple tools or APIs.\n",
       "- Smol-agent is ideal for multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls\n",
       "via executable Python code in sandbox environments.\n",
       "\n",
       "**Performance & Scalability:**\n",
       "- LangChain depends on the LLM and data store choice; it supports asynchronous workflows and horizontal scaling but\n",
       "may incur runtime overhead due to multiple chain steps.\n",
       "- Smol-agent performance depends on model speed and code complexity; generating and running code introduces some \n",
       "latency but can offload complex tasks efficiently.\n",
       "\n",
       "**Community & Maturity:**\n",
       "- LangChain has a large, active community, widespread adoption, and many integrations.\n",
       "- Smol-agent is newer, experimental, with a smaller but growing community, focusing on open-source flexibility.\n",
       "\n",
       "**Use Cases Comparison:**\n",
       "- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and workflow \n",
       "orchestration.\n",
       "- Use smol-agent when tasks demand precise code execution, advanced math, data parsing, or when preferring \n",
       "open-source code-centric workflows.\n",
       "\n",
       "**Example Contexts:**\n",
       "- For large document retrieval Q&A, LangChain or LlamaIndex are straightforward,\n",
       "- For AI agents that execute API calls or compute complex operations, smol-agent shines by code generation.\n",
       "\n",
       "**Limitations:**\n",
       "- LangChain may have a steep learning curve and runtime latency.\n",
       "- Smol-agent requires careful sandboxing and error handling for generated code; memory features for chatbots are \n",
       "limited.\n",
       "\n",
       "### 3. Additional context (if relevant):\n",
       "Many projects combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for \n",
       "conversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within broader \n",
       "pipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning, preference for \n",
       "code vs prompt-based logic, and production maturity needs.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 26.73 seconds| Input tokens: 2,098 | Output tokens: 161]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 26.73 seconds| Input tokens: 2,098 | Output tokens: 161]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Output message of the LLM:</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: I have detailed information about the differences between smol-agent and LangChain and their respective </span><span style=\"background-color: #0d1117\">  </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">use cases from the managed search agent. I will now summarize this information clearly, highlighting the main </span><span style=\"background-color: #0d1117\">     </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">differences and the scenarios in which each one is more suitable, and provide it as the final answer.</span><span style=\"background-color: #0d1117\">              </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">&lt;code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                             </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">summary = (</span><span style=\"background-color: #0d1117\">                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"smol-agent and LangChain differ primarily in design philosophy and use cases:\\n\\n\"</span><span style=\"background-color: #0d1117\">                            </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory,</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">and tool integration. \"</span><span style=\"background-color: #0d1117\">                                                                                            </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"It is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">orchestrating multiple APIs or services.\\n\"</span><span style=\"background-color: #0d1117\">                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step </span><span style=\"background-color: #0d1117\">  </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">task solving, especially useful for advanced logic, calculations, \"</span><span style=\"background-color: #0d1117\">                                                </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"and data manipulation in sandboxed environments.\\n\\n\"</span><span style=\"background-color: #0d1117\">                                                         </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"Use Cases:\\n\"</span><span style=\"background-color: #0d1117\">                                                                                                 </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool </span><span style=\"background-color: #0d1117\">      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">workflows, and scalable chatbot applications with memory.\\n\"</span><span style=\"background-color: #0d1117\">                                                       </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you </span><span style=\"background-color: #0d1117\">  </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">prefer an open-source, code-based approach to reasoning.\\n\\n\"</span><span style=\"background-color: #0d1117\">                                                      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">    \"In summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">shines in computation-heavy, code-driven problem solving.\"</span><span style=\"background-color: #0d1117\">                                                         </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">)</span><span style=\"background-color: #0d1117\">                                                                                                                  </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">final_answer(summary)</span><span style=\"background-color: #0d1117\">                                                                                              </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">&lt;/code&gt;</span><span style=\"background-color: #0d1117\">                                                                                                            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3mOutput message of the LLM:\u001b[0m \u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhave\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetailed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbetween\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtheir\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrespective\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcases\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmanaged\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msearch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magent.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msummarize\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mclearly,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhighlighting\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m     \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mdifferences\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscenarios\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhich\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23meach\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mone\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuitable,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovide\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mas\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfinal\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23manswer.\u001b[0m\u001b[48;2;13;17;23m              \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m<code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                             \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23msummary\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[48;2;13;17;23m                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"smol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdiffer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprimarily\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdesign\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mphilosophy\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcases:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                            \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodular\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mframework\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdesigned\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbuilding\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLM-driven\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapplications\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwith\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mflexible\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchains,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory,\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mintegration.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                                                            \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"It\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mideal\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflows,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchatbot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdevelopment,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconversational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23morchestrating\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmultiple\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mAPIs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mservices.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfocuses\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLLM-generated\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mPython\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexecution,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23menabling\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtransparent,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode-centric,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-step\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mtask\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msolving,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mespecially\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23museful\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madvanced\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlogic,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcalculations,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"and\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmanipulation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msandboxed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23menvironments.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                         \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"Use\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mCases:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                                                                 \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mUse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhen\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23myou\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconversational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magents,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieval-augmented\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgeneration,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmulti-tool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mworkflows,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mscalable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mchatbot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapplications\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwith\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                       \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"-\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mUse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhen\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23myour\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtasks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrequire\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprecise\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexecution,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23madvanced\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmath\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mparsing,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhen\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23myou\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m  \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprefer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23man\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mopen-source,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode-based\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapproach\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mreasoning.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"\u001b[0m\u001b[48;2;13;17;23m                                                      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\"In\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msummary,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mLangChain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mexcels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mworkflow\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23morchestration\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconversation-centric\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapplications,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhile\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msmol-agent\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mshines\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputation-heavy,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcode-driven\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mproblem\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msolving.\"\u001b[0m\u001b[48;2;13;17;23m                                                         \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                  \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mfinal_answer(summary)\u001b[0m\u001b[48;2;13;17;23m                                                                                              \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m</code>\u001b[0m\u001b[48;2;13;17;23m                                                                                                            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing parsed code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">summary </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> (</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"smol-agent and LangChain differ primarily in design philosophy and use cases:</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"background-color: #272822\">                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">memory, and tool integration. \"</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"It is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">orchestrating multiple APIs or services.</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"background-color: #272822\">                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, </span><span style=\"background-color: #272822\">         </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">multi-step task solving, especially useful for advanced logic, calculations, \"</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"and data manipulation in sandboxed environments.</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"background-color: #272822\">                                                     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Use Cases:</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"background-color: #272822\">                                                                                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">workflows, and scalable chatbot applications with memory.</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">you prefer an open-source, code-based approach to reasoning.</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">\\n\\n</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"</span><span style=\"background-color: #272822\">                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"In summary, LangChain excels at workflow orchestration and conversation-centric applications, while </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">smol-agent shines in computation-heavy, code-driven problem solving.\"</span><span style=\"background-color: #272822\">                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(summary)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting parsed code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34msmol-agent and LangChain differ primarily in design philosophy and use cases:\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mmemory, and tool integration. \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mIt is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34morchestrating multiple APIs or services.\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, \u001b[0m\u001b[48;2;39;40;34m         \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mmulti-step task solving, especially useful for advanced logic, calculations, \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mand data manipulation in sandboxed environments.\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mUse Cases:\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mworkflows, and scalable chatbot applications with memory.\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34myou prefer an open-source, code-based approach to reasoning.\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m\\n\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mIn summary, LangChain excels at workflow orchestration and conversation-centric applications, while \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34msmol-agent shines in computation-heavy, code-driven problem solving.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: smol-agent and LangChain differ primarily in design philosophy and use cases:</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory, and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">tool integration. It is ideal for multi-step reasoning workflows, complex chatbot development, conversational </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory, and orchestrating multiple APIs or services.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step task </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">solving, especially useful for advanced logic, calculations, and data manipulation in sandboxed environments.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Use Cases:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool workflows, </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">and scalable chatbot applications with memory.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you prefer </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">an open-source, code-based approach to reasoning.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">In summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">shines in computation-heavy, code-driven problem solving.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: smol-agent and LangChain differ primarily in design philosophy and use cases:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory, and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtool integration. It is ideal for multi-step reasoning workflows, complex chatbot development, conversational \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemory, and orchestrating multiple APIs or services.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step task \u001b[0m\n",
       "\u001b[1;38;2;212;183;2msolving, especially useful for advanced logic, calculations, and data manipulation in sandboxed environments.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mUse Cases:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool workflows, \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mand scalable chatbot applications with memory.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you prefer \u001b[0m\n",
       "\u001b[1;38;2;212;183;2man open-source, code-based approach to reasoning.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mIn summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mshines in computation-heavy, code-driven problem solving.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 6.41 seconds| Input tokens: 5,109 | Output tokens: 460]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 6.41 seconds| Input tokens: 5,109 | Output tokens: 460]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ë¦¬ì„œì¹˜í•  ì§ˆë¬¸ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "question = \"smol-agentì™€ LangChainì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì´ê³ , ê°ê° ì–´ë–¤ ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•œê°€ìš”?\"\n",
    "\n",
    "print(f\"ğŸ¤” ì§ˆë¬¸: {question}\")\n",
    "print(\"\\nğŸš€ Manager Agentê°€ ë¦¬ì„œì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Manager Agentì—ê²Œ ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "final_report = manager_agent.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81e30fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "âœ¨ ìµœì¢… ë¦¬ì„œì¹˜ ë³´ê³ ì„œ âœ¨\n",
      "==============================\n",
      "smol-agent and LangChain differ primarily in design philosophy and use cases:\n",
      "\n",
      "- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory, and tool integration. It is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and orchestrating multiple APIs or services.\n",
      "- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step task solving, especially useful for advanced logic, calculations, and data manipulation in sandboxed environments.\n",
      "\n",
      "Use Cases:\n",
      "- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool workflows, and scalable chatbot applications with memory.\n",
      "- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you prefer an open-source, code-based approach to reasoning.\n",
      "\n",
      "In summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent shines in computation-heavy, code-driven problem solving.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*30)\n",
    "print(\"âœ¨ ìµœì¢… ë¦¬ì„œì¹˜ ë³´ê³ ì„œ âœ¨\")\n",
    "print(\"=\"*30)\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de2b56",
   "metadata": {},
   "source": [
    "### Agent íŒ€ì˜ 'í˜‘ì—… ê³¼ì •' ì—¿ë³´ê¸°\n",
    "\n",
    "Managerì™€ Workerê°€ ì–´ë–¤ ëŒ€í™”ë¥¼ ì£¼ê³ ë°›ì•˜ê³ , Workerê°€ ì–´ë–¤ ë„êµ¬ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í–ˆëŠ”ì§€ manager_agent.memoryë¥¼ í†µí•´ ìƒì„¸í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ Agentì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996969b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "ğŸ§  Agent íŒ€ì˜ í˜‘ì—… ê³¼ì • (Memory) ì—¿ë³´ê¸°\n",
      "==============================\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', '<code>', and 'Observation:' sequences.\\n\\nAt each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\\nThen in the '<code>' sequence, you should write the code in simple Python. The code sequence must end with '</code>' sequence.\\nDuring each intermediate step, you can use 'print()' to save whatever important information you will then need.\\nThese print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.\\nIn the end you have to return a final answer using the `final_answer` tool.\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \\\"Generate an image of the oldest person in this document.\\\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\n<code>\\nanswer = document_qa(document=document, question=\\\"Who is the oldest person mentioned?\\\")\\nprint(answer)\\n</code>\\nObservation: \\\"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\\\"\\n\\nThought: I will now generate an image showcasing the oldest person.\\n<code>\\nimage = image_generator(\\\"A portrait of John Doe, a 55-year-old man living in Canada.\\\")\\nfinal_answer(image)\\n</code>\\n\\n---\\nTask: \\\"What is the result of the following operation: 5 + 3 + 1294.678?\\\"\\n\\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\\n<code>\\nresult = 5 + 3 + 1294.678\\nfinal_answer(result)\\n</code>\\n\\n---\\nTask:\\n\\\"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\\nYou have been provided with these additional arguments, that you can access using the keys as variables in your python code:\\n{'question': 'Quel est l'animal sur l'image?', 'image': 'path/to/image.jpg'}\\\"\\n\\nThought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\\n<code>\\ntranslated_question = translator(question=question, src_lang=\\\"French\\\", tgt_lang=\\\"English\\\")\\nprint(f\\\"The translated question is {translated_question}.\\\")\\nanswer = image_qa(image=image, question=translated_question)\\nfinal_answer(f\\\"The answer is {answer}\\\")\\n</code>\\n\\n---\\nTask:\\nIn a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\\nWhat does he say was the consequence of Einstein learning too much math on his creativity, in one word?\\n\\nThought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\\n<code>\\npages = web_search(query=\\\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\\\")\\nprint(pages)\\n</code>\\nObservation:\\nNo result found for query \\\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\\\".\\n\\nThought: The query was maybe too restrictive and did not find any results. Let's try again with a broader query.\\n<code>\\npages = web_search(query=\\\"1979 interview Stanislaus Ulam\\\")\\nprint(pages)\\n</code>\\nObservation:\\nFound 6 pages:\\n[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\\n\\n[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\\n\\n(truncated)\\n\\nThought: I will read the first 2 pages to know more.\\n<code>\\nfor url in [\\\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\\\", \\\"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\\\"]:\\n    whole_page = visit_webpage(url)\\n    print(whole_page)\\n    print(\\\"\\\\n\\\" + \\\"=\\\"*80 + \\\"\\\\n\\\")  # Print separator between pages\\n</code>\\nObservation:\\nManhattan Project Locations:\\nLos Alamos, NM\\nStanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\\n(truncated)\\n\\nThought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \\\"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\\\" Let's answer in one word.\\n<code>\\nfinal_answer(\\\"diminished\\\")\\n</code>\\n\\n---\\nTask: \\\"Which city has the highest population: Guangzhou or Shanghai?\\\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `web_search` to get the population of both cities.\\n<code>\\nfor city in [\\\"Guangzhou\\\", \\\"Shanghai\\\"]:\\n    print(f\\\"Population {city}:\\\", web_search(f\\\"{city} population\\\")\\n</code>\\nObservation:\\nPopulation Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\\nPopulation Shanghai: '26 million (2019)'\\n\\nThought: Now I know that Shanghai has the highest population.\\n<code>\\nfinal_answer(\\\"Shanghai\\\")\\n</code>\\n\\n---\\nTask: \\\"What is the current age of the pope, raised to the power 0.36?\\\"\\n\\nThought: I will use the tool `wikipedia_search` to get the age of the pope, and confirm that with a web search.\\n<code>\\npope_age_wiki = wikipedia_search(query=\\\"current pope age\\\")\\nprint(\\\"Pope age as per wikipedia:\\\", pope_age_wiki)\\npope_age_search = web_search(query=\\\"current pope age\\\")\\nprint(\\\"Pope age as per google search:\\\", pope_age_search)\\n</code>\\nObservation:\\nPope age: \\\"The pope Francis is currently 88 years old.\\\"\\n\\nThought: I know that the pope is 88 years old. Let's compute the result using python code.\\n<code>\\npope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\\n</code>\\n\\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions:\\n```python\\ndef final_answer(answer: any) -> any:\\n    \\\"\\\"\\\"Provides a final answer to the given problem.\\n\\n    Args:\\n        answer: The final answer to the problem\\n    \\\"\\\"\\\"\\n\\n```\\nYou can also give tasks to team members.\\nCalling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.\\nYou can also include any relevant variables or context using the 'additional_args' argument.\\nHere is a list of the team members that you can call:\\n```python\\ndef search_agent(task: str, additional_args: dict[str, Any]) -> str:\\n    \\\"\\\"\\\"ì›¹ ê²€ìƒ‰ ë° í˜ì´ì§€ ë°©ë¬¸ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ë¦¬ì„œì²˜ì…ë‹ˆë‹¤. íŠ¹ì • ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì´ Agentì—ê²Œ ëª…í™•í•œ ì§€ì‹œë¥¼ ë‚´ë¦¬ì„¸ìš”.\\n\\n    Args:\\n        task: Long detailed description of the task.\\n        additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.\\n    \\\"\\\"\\\"\\n\\n```\\n\\nHere are the rules you should always follow to solve your task:\\n1. Always provide a 'Thought:' sequence, and a '<code>' sequence ending with '</code>', else you will fail.\\n2. Use only variables that you have defined!\\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wikipedia_search({'query': \\\"What is the place where James Bond lives?\\\"})', but use the arguments directly as in 'answer = wikipedia_search(query=\\\"What is the place where James Bond lives?\\\")'.\\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to wikipedia_search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\\n6. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.\\n7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\\n8. You can use imports in your code, but only from the following list of modules: ['collections', 'datetime', 'itertools', 'math', 'queue', 'random', 're', 'stat', 'statistics', 'time', 'unicodedata']\\n9. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.\\n10. Don't give up! You're in charge of solving the task, not providing directions to solve it.\\n\\nNow Begin!\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"New task:\\nsmol-agentì™€ LangChainì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì´ê³ , ê°ê° ì–´ë–¤ ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•œê°€ìš”?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Thought: The user wants to know the main differences between smol-agent and LangChain and the situations where each is more suitable. This is a comparative question about two software/tools, likely related to AI or language models. I should search for reliable information or documentation about both smol-agent and LangChain to understand their key features and use cases. I will ask the search_agent to gather information on the differences and suitable use cases for both smol-agent and LangChain.\\n\\n<code>\\ntask_description = (\\n    \\\"Please provide detailed information on the main differences between smol-agent and LangChain. \\\"\\n    \\\"Additionally, explain in which scenarios or use cases each of these tools is more appropriate to use.\\\"\\n)\\n\\ninfo = search_agent(task=task_description, additional_args={})\\nprint(info)\\n</code>\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool-call\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Calling tools:\\n[{'id': 'call_1', 'type': 'function', 'function': {'name': 'python_interpreter', 'arguments': 'task_description = (\\\\n    \\\"Please provide detailed information on the main differences between smol-agent and LangChain. \\\"\\\\n    \\\"Additionally, explain in which scenarios or use cases each of these tools is more appropriate to use.\\\"\\\\n)\\\\n\\\\ninfo = search_agent(task=task_description, additional_args={})\\\\nprint(info)'}}]\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool-response\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Observation:\\nExecution logs:\\nHere is the final answer from your managed agent 'search_agent':\\n### 1. Task outcome (short version):\\nThe main differences between smol-agent and LangChain lie in their design philosophy, usage approach, and target use cases. LangChain is a modular framework for building LLM-driven applications with flexible chains, memory, and tool usage, excelling in multi-step reasoning and chatbot development. Smol-agent (by Hugging Face) focuses on LLMs generating and executing Python code for task-solving, offering transparency and power for code-based reasoning and multi-step computations, suited for open-source enthusiasts and advanced logic tasks.\\n\\n### 2. Task outcome (extremely detailed version):\\n\\n**Design Philosophies:**\\n- LangChain is modular, providing building blocks for LLM interactions, memory modules, tool usage, and chaining operations. It is the â€œglueâ€ for orchestrating complex workflows involving LLMs, vector stores, and APIs.\\n- Smol-agent uses a code-generation paradigm where the LLM generates Python code snippets dynamically to solve tasks by directly calling Python functions as tools, enabling transparent and debuggable multi-step reasoning.\\n\\n**Strengths & Usage:**\\n- LangChain excels at building conversational agents, chatbots with memory, retrieval-augmented generation workflows, and scenarios requiring orchestration of multiple tools or APIs.\\n- Smol-agent is ideal for multi-step tasks requiring calculations, data manipulation, advanced logic, or tool calls via executable Python code in sandbox environments.\\n\\n**Performance & Scalability:**\\n- LangChain depends on the LLM and data store choice; it supports asynchronous workflows and horizontal scaling but may incur runtime overhead due to multiple chain steps.\\n- Smol-agent performance depends on model speed and code complexity; generating and running code introduces some latency but can offload complex tasks efficiently.\\n\\n**Community & Maturity:**\\n- LangChain has a large, active community, widespread adoption, and many integrations.\\n- Smol-agent is newer, experimental, with a smaller but growing community, focusing on open-source flexibility.\\n\\n**Use Cases Comparison:**\\n- Use LangChain for chatbots requiring complex conversational memory, multi-step tool usage, and workflow orchestration.\\n- Use smol-agent when tasks demand precise code execution, advanced math, data parsing, or when preferring open-source code-centric workflows.\\n\\n**Example Contexts:**\\n- For large document retrieval Q&A, LangChain or LlamaIndex are straightforward,\\n- For AI agents that execute API calls or compute complex operations, smol-agent shines by code generation.\\n\\n**Limitations:**\\n- LangChain may have a steep learning curve and runtime latency.\\n- Smol-agent requires careful sandboxing and error handling for generated code; memory features for chatbots are limited.\\n\\n### 3. Additional context (if relevant):\\nMany projects combine these frameworks; e.g., LlamaIndex for scalable document retrieval alongside LangChain for conversational orchestration, or integrating smol-agent for specialized code-based reasoning tasks within broader pipelines. The choice depends on your projectâ€™s priorities: data scale, complexity of reasoning, preference for code vs prompt-based logic, and production maturity needs.\\nLast output from code snippet:\\nNone\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Thought: I have detailed information about the differences between smol-agent and LangChain and their respective use cases from the managed search agent. I will now summarize this information clearly, highlighting the main differences and the scenarios in which each one is more suitable, and provide it as the final answer.\\n\\n<code>\\nsummary = (\\n    \\\"smol-agent and LangChain differ primarily in design philosophy and use cases:\\\\n\\\\n\\\"\\n    \\\"- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory, and tool integration. \\\"\\n    \\\"It is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and orchestrating multiple APIs or services.\\\\n\\\"\\n    \\\"- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step task solving, especially useful for advanced logic, calculations, \\\"\\n    \\\"and data manipulation in sandboxed environments.\\\\n\\\\n\\\"\\n    \\\"Use Cases:\\\\n\\\"\\n    \\\"- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool workflows, and scalable chatbot applications with memory.\\\\n\\\"\\n    \\\"- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you prefer an open-source, code-based approach to reasoning.\\\\n\\\\n\\\"\\n    \\\"In summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent shines in computation-heavy, code-driven problem solving.\\\"\\n)\\n\\nfinal_answer(summary)\\n</code>\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool-call\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Calling tools:\\n[{'id': 'call_2', 'type': 'function', 'function': {'name': 'python_interpreter', 'arguments': 'summary = (\\\\n    \\\"smol-agent and LangChain differ primarily in design philosophy and use cases:\\\\\\\\n\\\\\\\\n\\\"\\\\n    \\\"- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory, and tool integration. \\\"\\\\n    \\\"It is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and orchestrating multiple APIs or services.\\\\\\\\n\\\"\\\\n    \\\"- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step task solving, especially useful for advanced logic, calculations, \\\"\\\\n    \\\"and data manipulation in sandboxed environments.\\\\\\\\n\\\\\\\\n\\\"\\\\n    \\\"Use Cases:\\\\\\\\n\\\"\\\\n    \\\"- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool workflows, and scalable chatbot applications with memory.\\\\\\\\n\\\"\\\\n    \\\"- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you prefer an open-source, code-based approach to reasoning.\\\\\\\\n\\\\\\\\n\\\"\\\\n    \\\"In summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent shines in computation-heavy, code-driven problem solving.\\\"\\\\n)\\\\n\\\\nfinal_answer(summary)'}}]\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool-response\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Observation:\\nExecution logs:\\nLast output from code snippet:\\nsmol-agent and LangChain differ primarily in design philosophy and use cases:\\n\\n- LangChain is a modular framework designed for building LLM-driven applications with flexible chains, memory, and tool integration. It is ideal for multi-step reasoning workflows, complex chatbot development, conversational memory, and orchestrating multiple APIs or services.\\n- smol-agent focuses on LLM-generated Python code execution, enabling transparent, code-centric, multi-step task solving, especially useful for advanced logic, calculations, and data manipulation in sandboxed environments.\\n\\nUse Cases:\\n- Use LangChain when you need complex conversational agents, retrieval-augmented generation, multi-tool workflows, and scalable chatbot applications with memory.\\n- Use smol-agent when your tasks require precise code execution, advanced math or data parsing, or when you prefer an open-source, code-based approach to reasoning.\\n\\nIn summary, LangChain excels at workflow orchestration and conversation-centric applications, while smol-agent shines in computation-heavy, code-driven problem solving.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. Agentì˜ ì „ì²´ ì‘ì—… ê¸°ë¡ì„ ChatMessage ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "agent_transcript_objects = manager_agent.write_memory_to_messages()\n",
    "\n",
    "# 2. JSONìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë„ë¡ ChatMessage ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ dictionary ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "transcript_as_dicts = []\n",
    "for msg in agent_transcript_objects:\n",
    "    # ëª¨ë“  ë©”ì‹œì§€ê°€ íŠ¹ì • ì†ì„±ì„ ê°€ì§€ê³  ìˆì§€ëŠ” ì•Šìœ¼ë¯€ë¡œ, hasattrë¡œ í™•ì¸ í›„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    message_dict = {\n",
    "        'role': msg.role,\n",
    "        'content': msg.content if hasattr(msg, 'content') and msg.content is not None else ''\n",
    "    }\n",
    "    \n",
    "    # ë„êµ¬ í˜¸ì¶œ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        message_dict['tool_calls'] = msg.tool_calls\n",
    "        \n",
    "    if hasattr(msg, 'tool_call_id') and msg.tool_call_id:\n",
    "        message_dict['tool_call_id'] = msg.tool_call_id\n",
    "        \n",
    "    transcript_as_dicts.append(message_dict)\n",
    "\n",
    "\n",
    "# 3. ì´ì œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ëŠ” JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "print(\"=\"*30)\n",
    "print(\"ğŸ§  Agent íŒ€ì˜ í˜‘ì—… ê³¼ì • (Memory) ì—¿ë³´ê¸°\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(json.dumps(transcript_as_dicts, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113906d",
   "metadata": {},
   "source": [
    "#### ìœ„ ì½”ë“œ ë² ì´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê²ƒë“¤ì„ ë¶™ì´ê³ , ë‚˜ë§Œì˜ Open Deep Research Agentë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
